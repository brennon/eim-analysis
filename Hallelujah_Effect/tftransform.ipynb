{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# \"Hallelujah Effect\" Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade --force tensorflow_transform==0.6.0 apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Restart the kernel</b> after you do a pip install (click on the <b>Reset</b> button in Datalab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-airflow==1.9.0\n",
      "apache-beam==2.4.0\n",
      "tensorflow==1.8.0\n",
      "tensorflow-transform==0.6.0\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip freeze | grep -e 'flow\\|beam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set bucket, project, and region\n",
    "BUCKET = 'eim-muse'\n",
    "PROJECT = 'eim-muse'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Retrieve and Subset Datasource\n",
    "\n",
    "Get data from BigQuery but defer filtering, etc. to Beam. Data in BigQuery has been pre-processed with Dataprep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "def create_query(phase, EVERY_N):\n",
    "  \"\"\"\n",
    "  phase: 1=train 2=valid\n",
    "  \"\"\"\n",
    "  base_query = \"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  `eim-muse.hallelujah_effect.full_hallelujah_trials_cleaned`\n",
    "  \"\"\"\n",
    "\n",
    "  if EVERY_N == None:\n",
    "    if phase < 2:\n",
    "      # Training\n",
    "      query = \"{0} WHERE MOD(FARM_FINGERPRINT(id), 10) < 7\".format(base_query)\n",
    "    else:\n",
    "      # Validation\n",
    "      query = \"{0} WHERE MOD(FARM_FINGERPRINT(id), 10) >= 8\".format(base_query)\n",
    "  else:\n",
    "      query = \"{0} WHERE MOD(FARM_FINGERPRINT(id), {1}) = {2}\".format(base_query, EVERY_N, phase)\n",
    "    \n",
    "  return query\n",
    "\n",
    "query = create_query(2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>concentration</th>\n",
       "      <th>musical_expertise</th>\n",
       "      <th>artistic</th>\n",
       "      <th>fault</th>\n",
       "      <th>imagination</th>\n",
       "      <th>lazy</th>\n",
       "      <th>nervous</th>\n",
       "      <th>outgoing</th>\n",
       "      <th>reserved</th>\n",
       "      <th>...</th>\n",
       "      <th>music_pref_none</th>\n",
       "      <th>music_pref_hiphop</th>\n",
       "      <th>music_pref_dance</th>\n",
       "      <th>music_pref_world</th>\n",
       "      <th>music_pref_rock</th>\n",
       "      <th>music_pref_pop</th>\n",
       "      <th>music_pref_classical</th>\n",
       "      <th>music_pref_jazz</th>\n",
       "      <th>music_pref_folk</th>\n",
       "      <th>music_pref_traditional_irish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.465116</td>\n",
       "      <td>4.043465</td>\n",
       "      <td>2.473455</td>\n",
       "      <td>2.518737</td>\n",
       "      <td>3.050490</td>\n",
       "      <td>3.752754</td>\n",
       "      <td>3.602112</td>\n",
       "      <td>3.626683</td>\n",
       "      <td>3.172583</td>\n",
       "      <td>3.157408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.046512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.033173</td>\n",
       "      <td>0.754667</td>\n",
       "      <td>1.088144</td>\n",
       "      <td>0.916852</td>\n",
       "      <td>0.953747</td>\n",
       "      <td>0.897524</td>\n",
       "      <td>1.034324</td>\n",
       "      <td>0.793569</td>\n",
       "      <td>1.085615</td>\n",
       "      <td>1.025196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152499</td>\n",
       "      <td>0.257770</td>\n",
       "      <td>0.411625</td>\n",
       "      <td>0.293903</td>\n",
       "      <td>0.504685</td>\n",
       "      <td>0.411625</td>\n",
       "      <td>0.441481</td>\n",
       "      <td>0.411625</td>\n",
       "      <td>0.213083</td>\n",
       "      <td>0.213083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>3.991266</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.412281</td>\n",
       "      <td>3.659389</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.529880</td>\n",
       "      <td>2.353712</td>\n",
       "      <td>3.144737</td>\n",
       "      <td>3.824561</td>\n",
       "      <td>3.659389</td>\n",
       "      <td>3.596491</td>\n",
       "      <td>3.228070</td>\n",
       "      <td>3.117904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             age  concentration  musical_expertise   artistic      fault  \\\n",
       "count  43.000000      43.000000          43.000000  43.000000  43.000000   \n",
       "mean   23.465116       4.043465           2.473455   2.518737   3.050490   \n",
       "std    11.033173       0.754667           1.088144   0.916852   0.953747   \n",
       "min     1.000000       2.000000           1.000000   1.000000   1.000000   \n",
       "25%    16.000000       3.991266           2.000000   2.000000   2.500000   \n",
       "50%    21.000000       4.000000           2.529880   2.353712   3.144737   \n",
       "75%    30.000000       4.000000           3.000000   3.000000   4.000000   \n",
       "max    45.000000       5.000000           5.000000   5.000000   5.000000   \n",
       "\n",
       "       imagination       lazy    nervous   outgoing   reserved  \\\n",
       "count    43.000000  43.000000  43.000000  43.000000  43.000000   \n",
       "mean      3.752754   3.602112   3.626683   3.172583   3.157408   \n",
       "std       0.897524   1.034324   0.793569   1.085615   1.025196   \n",
       "min       1.000000   1.000000   2.000000   1.000000   1.000000   \n",
       "25%       3.412281   3.659389   3.000000   3.000000   3.000000   \n",
       "50%       3.824561   3.659389   3.596491   3.228070   3.117904   \n",
       "75%       4.000000   4.000000   4.000000   4.000000   4.000000   \n",
       "max       5.000000   5.000000   5.000000   5.000000   5.000000   \n",
       "\n",
       "                   ...               music_pref_none  music_pref_hiphop  \\\n",
       "count              ...                     43.000000          43.000000   \n",
       "mean               ...                      0.023256           0.069767   \n",
       "std                ...                      0.152499           0.257770   \n",
       "min                ...                      0.000000           0.000000   \n",
       "25%                ...                      0.000000           0.000000   \n",
       "50%                ...                      0.000000           0.000000   \n",
       "75%                ...                      0.000000           0.000000   \n",
       "max                ...                      1.000000           1.000000   \n",
       "\n",
       "       music_pref_dance  music_pref_world  music_pref_rock  music_pref_pop  \\\n",
       "count         43.000000         43.000000        43.000000       43.000000   \n",
       "mean           0.209302          0.093023         0.534884        0.790698   \n",
       "std            0.411625          0.293903         0.504685        0.411625   \n",
       "min            0.000000          0.000000         0.000000        0.000000   \n",
       "25%            0.000000          0.000000         0.000000        1.000000   \n",
       "50%            0.000000          0.000000         1.000000        1.000000   \n",
       "75%            0.000000          0.000000         1.000000        1.000000   \n",
       "max            1.000000          1.000000         1.000000        1.000000   \n",
       "\n",
       "       music_pref_classical  music_pref_jazz  music_pref_folk  \\\n",
       "count             43.000000        43.000000        43.000000   \n",
       "mean               0.255814         0.209302         0.046512   \n",
       "std                0.441481         0.411625         0.213083   \n",
       "min                0.000000         0.000000         0.000000   \n",
       "25%                0.000000         0.000000         0.000000   \n",
       "50%                0.000000         0.000000         0.000000   \n",
       "75%                0.500000         0.000000         0.000000   \n",
       "max                1.000000         1.000000         1.000000   \n",
       "\n",
       "       music_pref_traditional_irish  \n",
       "count                     43.000000  \n",
       "mean                       0.046512  \n",
       "std                        0.213083  \n",
       "min                        0.000000  \n",
       "25%                        0.000000  \n",
       "50%                        0.000000  \n",
       "75%                        0.000000  \n",
       "max                        1.000000  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = bq.Query(query).execute().result().to_dataframe()\n",
    "df_valid.head()\n",
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create ML dataset using tf.transform and Dataflow\n",
    "\n",
    "Let's use Cloud Dataflow to read in the BigQuery data and write it out as CSV files. Along the way, let's use tf.transform to do scaling and transforming. Using tf.transform allows us to save the metadata to ensure that the appropriate transformations get carried out during prediction as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%writefile requirements.txt\n",
    "tensorflow-transform==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching local job ... hang on\n",
      "Query:\n",
      "\n",
      "SELECT *\n",
      "FROM\n",
      "  `eim-muse.hallelujah_effect.full_hallelujah_trials_cleaned`\n",
      "   WHERE MOD(FARM_FINGERPRINT(id), 10) < 7\n",
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./preproc_tft/tmp/tftransform_tmp/d70a639cc65f426193b0ba3baf25fd02/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./preproc_tft/tmp/tftransform_tmp/d70a639cc65f426193b0ba3baf25fd02/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./preproc_tft/tmp/tftransform_tmp/66bfb6d19f5042a98f6d569f7864bff2/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./preproc_tft/tmp/tftransform_tmp/66bfb6d19f5042a98f6d569f7864bff2/saved_model.pb\n",
      "WARNING:root:Dataset eim-muse:temp_dataset_666a9a8382684c65b659e3e46a4e784b does not exist so we will create it as temporary with location=None\n",
      "WARNING:root:Dataset eim-muse:temp_dataset_5c8069b71639435b841d476745db69ab does not exist so we will create it as temporary with location=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./preproc_tft/tmp/tftransform_tmp/9ce85aa98a61418bba52571aee90b7b1/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./preproc_tft/tmp/tftransform_tmp/9ce85aa98a61418bba52571aee90b7b1/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-21 13:31:53.438231. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "\n",
    "def is_valid(inputs):\n",
    "  try:\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "  \n",
    "def preprocess_tft(inputs):\n",
    "      import datetime\n",
    "      result = {}\n",
    "      result['age'] = tft.scale_to_0_1(inputs['age'])      \n",
    "      result['activity'] = tft.scale_to_0_1(inputs['activity'])\n",
    "      result['hallelujah_reaction'] = tf.cast(inputs['hallelujah_reaction'], tf.int64)\n",
    "#       result['concentration'] = tft.scale_to_0_1(inputs['concentration'])\n",
    "      result['hearing_impairments'] = tf.cast(inputs['hearing_impairments'], tf.int64)\n",
    "      result['nationality'] = tf.identity(inputs['nationality'])\n",
    "      result['engagement'] = tft.scale_to_0_1(inputs['engagement'])\n",
    "      result['familiarity'] = tft.scale_to_0_1(inputs['familiarity'])\n",
    "      result['like_dislike'] = tft.scale_to_0_1(inputs['like_dislike'])\n",
    "      result['positivity'] = tft.scale_to_0_1(inputs['positivity'])\n",
    "      result['tension'] = tft.scale_to_0_1(inputs['tension'])\n",
    "      result['sex'] = tf.identity(inputs['sex'])\n",
    "      result['location'] = tf.identity(inputs['location'])\n",
    "      result['language'] = tf.identity(inputs['language'])\n",
    "      return result\n",
    "\n",
    "def preprocess(in_test_mode, EVERY_N=None):\n",
    "  import os\n",
    "  import os.path\n",
    "  import tempfile\n",
    "  from apache_beam.io import tfrecordio\n",
    "  from tensorflow_transform.coders import example_proto_coder\n",
    "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "  from tensorflow_transform.tf_metadata import dataset_schema\n",
    "  from tensorflow_transform.beam import tft_beam_io\n",
    "  from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "  job_name = 'hallelujah-effect-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')    \n",
    "  if in_test_mode:\n",
    "    import shutil\n",
    "    print 'Launching local job ... hang on'\n",
    "    OUTPUT_DIR = './preproc_tft'\n",
    "    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "    \n",
    "  else:\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "    OUTPUT_DIR = 'gs://{0}/analysis/hallelujah-effect/preproc_tft/'.format(BUCKET)\n",
    "    import subprocess\n",
    "    subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
    "  \n",
    "  # Configure Beam pipeline options\n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT,\n",
    "    'max_num_workers': 24,\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True,\n",
    "    'requirements_file': 'requirements.txt'\n",
    "  }\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  if in_test_mode:\n",
    "    RUNNER = 'DirectRunner'\n",
    "  else:\n",
    "    RUNNER = 'DataflowRunner'\n",
    "\n",
    "  # Setup metadata\n",
    "  raw_data_schema = {\n",
    "    colname : dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'nationality,sex,location,language'.split(',')\n",
    "  }\n",
    "  raw_data_schema.update({\n",
    "      colname : dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'age,activity,engagement,familiarity,like_dislike,positivity,tension'.split(',')\n",
    "    })\n",
    "  raw_data_schema.update({\n",
    "      colname : dataset_schema.ColumnSchema(tf.bool, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'hallelujah_reaction,hearing_impairments'.split(',')\n",
    "    })\n",
    "  raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
    "  \n",
    "  # run Beam  \n",
    "  with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "    with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
    "      \n",
    "      # Write the raw data metadata to disk\n",
    "      # Without the overloaded operators: raw_data_metadata.apply(tft_beam_io.WriteMetadata(os.path.join(OUTPUT_DIR, 'metadata/rawdata_metadata'), pipeline=p, label='WriteInputMetadata')\n",
    "      _ = (raw_data_metadata\n",
    "        | 'WriteInputMetadata' >> tft_beam_io.WriteMetadata(\n",
    "            os.path.join(OUTPUT_DIR, 'metadata/rawdata_metadata'),\n",
    "            pipeline=p))\n",
    "           \n",
    "      # Analyze and transform training data\n",
    "      this_query = create_query(1, EVERY_N)\n",
    "      \n",
    "      print('Query:')\n",
    "      print(this_query)\n",
    "      \n",
    "      def debug_print(p_collection_as_list):\n",
    "        print(p_collection_as_list)\n",
    "      \n",
    "      # Read in training data from BigQuery table\n",
    "      raw_data = (p\n",
    "        # Get raw training data from BigQuery\n",
    "        | 'train_read' >> beam.io.Read(beam.io.BigQuerySource(query=this_query, use_standard_sql=True))\n",
    "        # Use our is_valid function to only retain valid examples from training data\n",
    "        | 'train_filter' >> beam.Filter(is_valid))\n",
    "\n",
    "      # Package raw training data and its metadata into a 'dataset'\n",
    "      raw_dataset = (raw_data, raw_data_metadata)\n",
    "      \n",
    "      # Using the preprocessing function `preprocess_tft`, preprocess the training data\n",
    "      # and produce a transformed training dataset and a function to transform other data later\n",
    "      transformed_dataset, transform_fn = (\n",
    "          raw_dataset | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))\n",
    "      \n",
    "      # Break out the transformed training data and its metadata\n",
    "      transformed_data, transformed_metadata = transformed_dataset\n",
    "      \n",
    "      # Write the transformed training data to files\n",
    "      _ = transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
    "          os.path.join(OUTPUT_DIR, 'train'),\n",
    "          file_name_suffix='.gz',\n",
    "          coder=example_proto_coder.ExampleProtoCoder(\n",
    "              transformed_metadata.schema))\n",
    "      \n",
    "      # Read in test data from BigQuery table and filter as we did with training data\n",
    "      raw_test_data = (p \n",
    "        | 'eval_read' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(2, EVERY_N), use_standard_sql=True))\n",
    "        | 'eval_filter' >> beam.Filter(is_valid))\n",
    "      \n",
    "      # Package test data and metadata into a dataset\n",
    "      raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
    "      \n",
    "      # Using the same transformation function that was calculated above, transform the test dataset\n",
    "      transformed_test_dataset = (\n",
    "          (raw_test_dataset, transform_fn) | beam_impl.TransformDataset())\n",
    "      \n",
    "      # Write the transformed test data to files\n",
    "      transformed_test_data, _ = transformed_test_dataset\n",
    "      _ = transformed_test_data | 'WriteTestData' >> tfrecordio.WriteToTFRecord(\n",
    "          os.path.join(OUTPUT_DIR, 'eval'),\n",
    "          file_name_suffix='.gz',\n",
    "          coder=example_proto_coder.ExampleProtoCoder(\n",
    "              transformed_metadata.schema))\n",
    "      \n",
    "      # Write the transformation function to a file, as well\n",
    "      _ = (transform_fn\n",
    "           | 'WriteTransformFn' >>\n",
    "           transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'metadata')))\n",
    "\n",
    "# Preprocess the training/test data\n",
    "preprocess(in_test_mode=True, EVERY_N=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "# ls -l preproc_tft\n",
    "# ls preproc_tft/metadata\n",
    "gsutil ls -l gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/\n",
    "gsutil ls gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2> Train off preprocessed data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'feature_engineering'\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf ${PWD}/models/${MODEL_NAME}\n",
    "export PYTHONPATH=${PYTHONPATH}:$PWD/taxifare_tft\n",
    "python -m trainer.task \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\"  \\\n",
    "   --train_batch_size=128 \\\n",
    "   --output_dir=${PWD}/models/${MODEL_NAME} \\\n",
    "   --train_steps=50000 --eval_steps=1 --job-dir=/tmp \\\n",
    "   --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata\n",
    "   --hidden_units=\"16 16 16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://eim-muse/analysis/hallelujah-effect/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TensorBoard.stop(20767)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf ${PWD}/models/local-ml\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/taxifare_tft/trainer \\\n",
    "   --job-dir=${PWD}/models/local-ml \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\" \\\n",
    "   --train_steps=1000 \\\n",
    "   --train_batch_size=10 \\\n",
    "   --eval_steps=100 \\\n",
    "   --output_dir=${PWD}/models/local-ml \\\n",
    "   --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata/\n",
    "\n",
    "# %%bash\n",
    "# OUTDIR=gs://${BUCKET}/analysis/hallelujah-effect/models/hallelujah-effect_trained\n",
    "# JOBNAME=hallelujah_effect$(date -u +%y%m%d_%H%M%S)\n",
    "# echo $OUTDIR $REGION $JOBNAME\n",
    "# gsutil -m rm -rf $OUTDIR\n",
    "# gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "#    --region=$REGION \\\n",
    "#    --package-path=${PWD}/taxifare/trainer \\\n",
    "#    --module-name=trainer.task \\\n",
    "#    --job-dir=$OUTDIR \\\n",
    "#    --scale-tier=STANDARD_1 \\\n",
    "#    --runtime-version=1.4 \\\n",
    "#    -- \\\n",
    "#    --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "#    --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\" \\\n",
    "#    --output_dir=$OUTDIR \\\n",
    "#    --train_steps=1000\n",
    "#    --train_batch_size=10 \\\n",
    "#    --eval_steps=100\n",
    "#    --config=hyperparam.yaml \\\n",
    "\n",
    "# --staging-bucket=gs://eim-muse-staging \\\n",
    "\n",
    "# export PYTHONPATH=${PYTHONPATH}:$PWD/taxifare_tft\n",
    "# python -m trainer.task \\\n",
    "#    --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "#    --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\"  \\\n",
    "#    --train_batch_size=10 \\\n",
    "#    --output_dir=\"gs://${BUCKET}/analysis/hallelujah-effect/models/hallelujah-effect_trained\" \\\n",
    "#    --train_steps=5000 --eval_steps=1 --job-dir=/tmp \\\n",
    "#    --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "# rm -rf ${PWD}/models/local-ml\n",
    "# gcloud ml-engine local train \\\n",
    "#    --module-name=trainer.task \\\n",
    "#    --package-path=${PWD}/taxifare_tft/trainer \\\n",
    "#    --job-dir=${PWD}/models/local-ml \\\n",
    "#    -- \\\n",
    "#    --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "#    --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\" \\\n",
    "#    --train_steps=1000 \\\n",
    "#    --train_batch_size=10 \\\n",
    "#    --eval_steps=100 \\\n",
    "#    --output_dir=${PWD}/models/local-ml \\\n",
    "#    --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata/\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/analysis/hallelujah-effect/models/hallelujah-effect_trained\n",
    "JOBNAME=hallelujah_effect$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --package-path=${PWD}/taxifare_tft/trainer \\\n",
    "   --module-name=trainer.task \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   --runtime-version=1.4 \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\" \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --train_steps=50000 \\\n",
    "   --train_batch_size=64 \\\n",
    "   --eval_steps=1 \\\n",
    "   --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata/\n",
    "\n",
    "#    --config=hyperparam.yaml\n",
    "# --staging-bucket=gs://eim-muse-staging \\\n",
    "\n",
    "# export PYTHONPATH=${PYTHONPATH}:$PWD/taxifare_tft\n",
    "# python -m trainer.task \\\n",
    "#    --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "#    --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\"  \\\n",
    "#    --train_batch_size=10 \\\n",
    "#    --output_dir=\"gs://${BUCKET}/analysis/hallelujah-effect/models/hallelujah-effect_trained\" \\\n",
    "#    --train_steps=5000 --eval_steps=1 --job-dir=/tmp \\\n",
    "#    --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TensorBoard().start('./models/local-ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls gs://eim-muse/analysis/hallelujah-effect/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%writefile /tmp/test.json\n",
    "{\"age\":\"29.0\",\"activity\":3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "model_dir=$(ls $PWD/hallelujah-effect_trained/export/exporter/)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=./hallelujah-effect_trained/export/exporter/${model_dir} \\\n",
    "    --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# To Do\n",
    "\n",
    "- LASSO to identify important features\n",
    "- Hyperparameter search\n",
    "- More plots and statistics from the dataset with which I'm working here\n",
    "- Bring in rows with missing values\n",
    "- Feature engineering (physiological signals, MIR, feature crosses, variable-width binning)\n",
    "- Include signals with good quality only in reaction range\n",
    "- Customize estimator to add additional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "cat ${PWD}/taxifare_tft/trainer/setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
