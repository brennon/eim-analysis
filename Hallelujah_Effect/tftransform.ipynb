{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1> Exploring tf.transform </h1>\n",
    "\n",
    "While Pandas is fine for experimenting, for operationalization of your workflow, it is better to do preprocessing in Apache Beam. This will also help if you need to preprocess data in flight, since Apache Beam also allows for streaming.\n",
    "\n",
    "Only specific combinations of TensorFlow/Beam are supported by tf.transform. So make sure to get a combo that is.\n",
    "\n",
    "* TFT 0.6.0\n",
    "* TF 1.6 or higher\n",
    "* Apache Beam [GCP] 2.4.0 or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling google-cloud-dataflow-2.0.0:\n",
      "  Successfully uninstalled google-cloud-dataflow-2.0.0\n",
      "Collecting tensorflow_transform==0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/e5/b1f69575d78936acc2a41344dadf753c005025e20f03afe38c43e08cc8f2/tensorflow-transform-0.6.0.tar.gz (113kB)\n",
      "Collecting apache-beam[gcp]\n",
      "  Downloading https://files.pythonhosted.org/packages/28/9f/e4be9a3fda95b34aca8e37ab53d355d346d493e7ebeac0e7d26ebeb7625d/apache_beam-2.4.0-cp27-cp27mu-manylinux1_x86_64.whl (2.1MB)\n",
      "Collecting numpy<2,>=1.10 (from tensorflow_transform==0.6.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/a9/c01a2d5f7b045f508c8cefef3b079fe8c413d05498ca0ae877cffa230564/numpy-1.14.5-cp27-cp27mu-manylinux1_x86_64.whl (12.1MB)\n",
      "Collecting protobuf<4,>=3.5.2 (from tensorflow_transform==0.6.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/9d/61/54c3a9cfde6ffe0ca6a1786ddb8874263f4ca32e7693ad383bd8cf935015/protobuf-3.5.2.post1-cp27-cp27mu-manylinux1_x86_64.whl (6.4MB)\n",
      "Collecting six<2,>=1.9 (from tensorflow_transform==0.6.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
      "Collecting httplib2<0.10,>=0.8 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/a9/5751cdf17a70ea89f6dde23ceb1705bfb638fd8cee00f845308bf8d26397/httplib2-0.9.2.tar.gz (205kB)\n",
      "Collecting oauth2client<5,>=2.0.1 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/82/d8/3eab58811282ac7271a081ba5c0d4b875ce786ca68ce43e2a62ade32e9a8/oauth2client-4.1.2-py2.py3-none-any.whl (99kB)\n",
      "Collecting typing<3.7.0,>=3.6.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/4d/4e5985d075d241d686a1663fa1f88b61d544658d08c1375c7c6aac32afc3/typing-3.6.4-py2-none-any.whl\n",
      "Collecting dill==0.2.6 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/69/0d03d5f9af0e16d41bb47262100b0c4c08f90538c9a5c2de0d44284172ba/dill-0.2.6.zip (83kB)\n",
      "Collecting grpcio<2,>=1.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/0d/146582f71161a0074dda2378617ae5f7e2c3d6cf62d4588eb586c1d6b675/grpcio-1.12.1-cp27-cp27mu-manylinux1_x86_64.whl (8.9MB)\n",
      "Collecting pyvcf<0.7.0,>=0.6.8 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/20/b6/36bfb1760f6983788d916096193fc14c83cce512c7787c93380e09458c09/PyVCF-0.6.8.tar.gz\n",
      "Collecting avro<2.0.0,>=1.8.1 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/27/143f124a7498f841317a92ced877150c5cb8d28a4109ec39666485925d00/avro-1.8.2.tar.gz (43kB)\n",
      "Collecting crcmod<2.0,>=1.7 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/6b/b0/e595ce2a2527e169c3bcd6c33d2473c1918e0b7f6826a043ca1245dd4e5b/crcmod-1.7.tar.gz (89kB)\n",
      "Collecting mock<3.0.0,>=1.0.1 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "Collecting futures<4.0.0,>=3.1.1 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/99/b2c4e9d5a30f6471e410a146232b4118e697fa3ffc06d6a65efde84debd0/futures-3.2.0-py2-none-any.whl\n",
      "Collecting pyyaml<4.0.0,>=3.12 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/85/db5a2df477072b2902b0eb892feb37d88ac635d36245a72a6a69b23b383a/PyYAML-3.12.tar.gz (253kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/ef/35af4764ea6c60bd14195ca78d4e831d154183f35cc4af4a0b3e01aa28ce/hdfs-2.1.0.tar.gz\n",
      "Collecting google-apitools<=0.5.20,>=0.5.18; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/0c/64f84f91643f775fdb64c6c10f4a4f0d827f8b0d98a2ba2b4bb9dc2f8646/google_apitools-0.5.20-py2-none-any.whl (330kB)\n",
      "Collecting google-cloud-bigquery==0.25.0; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/76/67/6165c516ff6ceaa62eb61f11d8451e1b0acc4d3775e181630aba9652babb/google_cloud_bigquery-0.25.0-py2.py3-none-any.whl (41kB)\n",
      "Collecting google-cloud-pubsub==0.26.0; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/37/92/c74a643126d58505daec9addf872dfaffea3305981b90cc435f4b9213cdd/google_cloud_pubsub-0.26.0-py2.py3-none-any.whl\n",
      "Collecting proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/1f/4124f15e1132a2eeeaf616d825990bb1d395b4c2c37362654ea5cd89bb42/proto-google-cloud-datastore-v1-0.90.4.tar.gz\n",
      "Collecting googledatastore==7.0.1; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/73/d0/17ce873331aaf529ab238464a15fd7bdc1ba8d2c684789970a7fa8b505a8/googledatastore-7.0.1.tar.gz\n",
      "Collecting setuptools (from protobuf<4,>=3.5.2->tensorflow_transform==0.6.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/e1/820d941153923aac1d49d7fc37e17b6e73bfbd2904959fffbad77900cf92/setuptools-39.2.0-py2.py3-none-any.whl (567kB)\n",
      "Collecting rsa>=3.1.4 (from oauth2client<5,>=2.0.1->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
      "Collecting pyasn1>=0.1.7 (from oauth2client<5,>=2.0.1->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/70/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9/pyasn1-0.4.3-py2.py3-none-any.whl (72kB)\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client<5,>=2.0.1->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/51/bcd96bf6231d4b2cc5e023c511bee86637ba375c44a6f9d1b4b7ad1ce4b9/pyasn1_modules-0.2.1-py2.py3-none-any.whl (60kB)\n",
      "Collecting enum34>=1.0.4 (from grpcio<2,>=1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock<3.0.0,>=1.0.1->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pbr>=0.11 (from mock<3.0.0,>=1.0.1->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/5d/c196041ffdf3e34ba206db6d61d1f893a75e1f3435699ade9bd65e089a3d/pbr-4.0.4-py2.py3-none-any.whl (98kB)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Collecting requests>=2.7.0 (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/15/e1c318dbc20032ffbe5628837ca0de2d5b116ffd1b849c699634010f6a5d/requests-2.19.0-py2.py3-none-any.whl (91kB)\n",
      "Collecting fasteners>=0.14 (from google-apitools<=0.5.20,>=0.5.18; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/14/3a/096c7ad18e102d4f219f5dd15951f9728ca5092a3385d2e8f79a7c1e1017/fasteners-0.14.1-py2.py3-none-any.whl\n",
      "Collecting google-cloud-core<0.26dev,>=0.25.0 (from google-cloud-bigquery==0.25.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/dd/00e90bd1f6788f06ca5ea83a0ec8dd76350b38303bb8f09d2bf692eb1294/google_cloud_core-0.25.0-py2.py3-none-any.whl (52kB)\n",
      "Collecting gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0 (from google-cloud-pubsub==0.26.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a7/0225bd7a95e037a0afa90b2dd9534d0c79cd62283a5bddb30a3197579cbc/gapic-google-cloud-pubsub-v1-0.15.4.tar.gz\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.5.2 (from proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/00/03/d25bed04ec8d930bcfa488ba81a2ecbf7eb36ae3ffd7e8f5be0d036a89c9/googleapis-common-protos-1.5.3.tar.gz\n",
      "Collecting idna<2.8,>=2.5 (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting urllib3<1.24,>=1.21.1 (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/e6/92ad559b7192d846975fc916b65f667c7b8c3a32bea7372340bfe9a15fa5/certifi-2018.4.16-py2.py3-none-any.whl (150kB)\n",
      "Collecting monotonic>=0.1 (from fasteners>=0.14->google-apitools<=0.5.20,>=0.5.18; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
      "Collecting google-auth-httplib2 (from google-cloud-core<0.26dev,>=0.25.0->google-cloud-bigquery==0.25.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/33/49/c814d6d438b823441552198f096fcd0377fd6c88714dbed34f1d3c8c4389/google_auth_httplib2-0.0.3-py2.py3-none-any.whl\n",
      "Collecting google-auth<2.0.0dev,>=0.4.0 (from google-cloud-core<0.26dev,>=0.25.0->google-cloud-bigquery==0.25.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/53/06/6e6d5bfa4d23ee40efd772d6b681a7afecd859a9176e564b8c329382370f/google_auth-1.5.0-py2.py3-none-any.whl (65kB)\n",
      "Collecting google-gax<0.16dev,>=0.15.7 (from gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/b4/ff312fa42f91535c67567c1d08e972db0e7c548e9a63c6f3bcc5213b32fc/google_gax-0.15.16-py2.py3-none-any.whl (46kB)\n",
      "Collecting proto-google-cloud-pubsub-v1[grpc]<0.16dev,>=0.15.4 (from gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/a2/2eeffa0069830f00016196dfdd69491cf562372b5353f2e8e378b3c2cb0a/proto-google-cloud-pubsub-v1-0.15.4.tar.gz\n",
      "Collecting grpc-google-iam-v1<0.12dev,>=0.11.1 (from gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/28/f26f67381cb23e81271b8d66c00a846ad9d25a909ae1ae1df8222fad2744/grpc-google-iam-v1-0.11.4.tar.gz\n",
      "Collecting cachetools>=2.0.0 (from google-auth<2.0.0dev,>=0.4.0->google-cloud-core<0.26dev,>=0.25.0->google-cloud-bigquery==0.25.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/58/cbee863250b31d80f47401d04f34038db6766f95dea1cc909ea099c7e571/cachetools-2.1.0-py2.py3-none-any.whl\n",
      "Collecting future<0.17dev,>=0.16.0 (from google-gax<0.16dev,>=0.15.7->gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/00/2b/8d082ddfed935f3608cc61140df6dcbf0edea1bc3ab52fb6c29ae3e81e85/future-0.16.0.tar.gz (824kB)\n",
      "Collecting ply==3.8 (from google-gax<0.16dev,>=0.15.7->gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/96/e0/430fcdb6b3ef1ae534d231397bee7e9304be14a47a267e82ebcb3323d0b5/ply-3.8.tar.gz (157kB)\n",
      "Building wheels for collected packages: tensorflow-transform, httplib2, dill, pyvcf, avro, crcmod, pyyaml, hdfs, proto-google-cloud-datastore-v1, googledatastore, docopt, gapic-google-cloud-pubsub-v1, googleapis-common-protos, proto-google-cloud-pubsub-v1, grpc-google-iam-v1, future, ply\n",
      "  Running setup.py bdist_wheel for tensorflow-transform: started\n",
      "  Running setup.py bdist_wheel for tensorflow-transform: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/67/67/f9/a5f81a500bbbddf2822eb4371681ea21f6ca4ac16d94e4fcdf\n",
      "  Running setup.py bdist_wheel for httplib2: started\n",
      "  Running setup.py bdist_wheel for httplib2: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/36/f2/49/5adbf90fba31e02a7784e1147d7f8b6c4af3718739e568c8cb\n",
      "  Running setup.py bdist_wheel for dill: started\n",
      "  Running setup.py bdist_wheel for dill: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/17/74/0a/250b442a5196d5c0d80f7d4259a0a26a9c9af0e6b0ee98a335\n",
      "  Running setup.py bdist_wheel for pyvcf: started\n",
      "  Running setup.py bdist_wheel for pyvcf: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/81/91/41/3272543c0b9c61da9c525f24ee35bae6fe8f60d4858c66805d\n",
      "  Running setup.py bdist_wheel for avro: started\n",
      "  Running setup.py bdist_wheel for avro: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/bf/0b/75/1b3517b7d36ddc8ba5d22c0df5eb01e83979f34420066d643e\n",
      "  Running setup.py bdist_wheel for crcmod: started\n",
      "  Running setup.py bdist_wheel for crcmod: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/50/24/4d/4580ca4a299f1ad6fd63443e6e584cb21e9a07988e4aa8daac\n",
      "  Running setup.py bdist_wheel for pyyaml: started\n",
      "  Running setup.py bdist_wheel for pyyaml: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/03/05/65/bdc14f2c6e09e82ae3e0f13d021e1b6b2481437ea2f207df3f\n",
      "  Running setup.py bdist_wheel for hdfs: started\n",
      "  Running setup.py bdist_wheel for hdfs: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/8c/89/59/b95e6bf2d5957541862f2cd109a052f26b61dc2fad2f04a1b4\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-datastore-v1: started\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-datastore-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/bd/ce/33/8b769968db3761c42c7a91d8a0dbbafc50acfa0750866c8abd\n",
      "  Running setup.py bdist_wheel for googledatastore: started\n",
      "  Running setup.py bdist_wheel for googledatastore: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/93/fa/12/99c490525fff40bcd240504f694bd02848479bc2debc31daae\n",
      "  Running setup.py bdist_wheel for docopt: started\n",
      "  Running setup.py bdist_wheel for docopt: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "  Running setup.py bdist_wheel for gapic-google-cloud-pubsub-v1: started\n",
      "  Running setup.py bdist_wheel for gapic-google-cloud-pubsub-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/f4/2b/10/bdcbc9be2ae4e437232e118056f026025cf2cc46d6dcf0d69d\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos: started\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/62/45/af/649bbf07b6595fda010be1bda667cd56d0444d07afc6f8b687\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-pubsub-v1: started\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-pubsub-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/d1/0d/9e/95e7192ab2625847ac40b2bc618800bf5b6c984cd572a83314\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1: started\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/b6/c6/31/c20321a5a3fde456fc375b7c2814135e6e98bc0d74c40239d9\n",
      "  Running setup.py bdist_wheel for future: started\n",
      "  Running setup.py bdist_wheel for future: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/bf/c9/a3/c538d90ef17cf7823fa51fc701a7a7a910a80f6a405bf15b1a\n",
      "  Running setup.py bdist_wheel for ply: started\n",
      "  Running setup.py bdist_wheel for ply: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/f2/21/c0/f0056cc96847933daa961a19eb59a2ecd0228fdbe3376e7a68\n",
      "Successfully built tensorflow-transform httplib2 dill pyvcf avro crcmod pyyaml hdfs proto-google-cloud-datastore-v1 googledatastore docopt gapic-google-cloud-pubsub-v1 googleapis-common-protos proto-google-cloud-pubsub-v1 grpc-google-iam-v1 future ply\n",
      "Installing collected packages: httplib2, six, pyasn1, rsa, pyasn1-modules, oauth2client, typing, dill, futures, enum34, grpcio, setuptools, pyvcf, avro, crcmod, funcsigs, pbr, mock, pyyaml, protobuf, docopt, idna, chardet, urllib3, certifi, requests, hdfs, monotonic, fasteners, google-apitools, googleapis-common-protos, cachetools, google-auth, google-auth-httplib2, google-cloud-core, google-cloud-bigquery, future, ply, google-gax, proto-google-cloud-pubsub-v1, grpc-google-iam-v1, gapic-google-cloud-pubsub-v1, google-cloud-pubsub, proto-google-cloud-datastore-v1, googledatastore, apache-beam, numpy, tensorflow-transform\n",
      "  Found existing installation: httplib2 0.11.3\n",
      "    Uninstalling httplib2-0.11.3:\n",
      "      Successfully uninstalled httplib2-0.11.3\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: pyasn1 0.4.2\n",
      "    Uninstalling pyasn1-0.4.2:\n",
      "      Successfully uninstalled pyasn1-0.4.2\n",
      "  Found existing installation: rsa 3.4.2\n",
      "    Uninstalling rsa-3.4.2:\n",
      "      Successfully uninstalled rsa-3.4.2\n",
      "  Found existing installation: pyasn1-modules 0.2.1\n",
      "    Uninstalling pyasn1-modules-0.2.1:\n",
      "      Successfully uninstalled pyasn1-modules-0.2.1\n",
      "  Found existing installation: oauth2client 2.2.0\n",
      "    Uninstalling oauth2client-2.2.0:\n",
      "      Successfully uninstalled oauth2client-2.2.0\n",
      "  Found existing installation: dill 0.2.7.1\n",
      "    Uninstalling dill-0.2.7.1:\n",
      "      Successfully uninstalled dill-0.2.7.1\n",
      "  Found existing installation: futures 3.2.0\n",
      "    Uninstalling futures-3.2.0:\n",
      "      Successfully uninstalled futures-3.2.0\n",
      "  Found existing installation: enum34 1.1.6\n",
      "    Uninstalling enum34-1.1.6:\n",
      "      Successfully uninstalled enum34-1.1.6\n",
      "  Found existing installation: grpcio 1.11.0\n",
      "    Uninstalling grpcio-1.11.0:\n",
      "      Successfully uninstalled grpcio-1.11.0\n",
      "  Found existing installation: setuptools 39.1.0\n",
      "    Uninstalling setuptools-39.1.0:\n",
      "      Successfully uninstalled setuptools-39.1.0\n",
      "  Found existing installation: avro 1.8.2\n",
      "    Uninstalling avro-1.8.2:\n",
      "      Successfully uninstalled avro-1.8.2\n",
      "  Found existing installation: crcmod 1.7\n",
      "    Uninstalling crcmod-1.7:\n",
      "      Successfully uninstalled crcmod-1.7\n",
      "  Found existing installation: funcsigs 1.0.0\n",
      "    Uninstalling funcsigs-1.0.0:\n",
      "      Successfully uninstalled funcsigs-1.0.0\n",
      "  Found existing installation: pbr 4.0.2\n",
      "    Uninstalling pbr-4.0.2:\n",
      "      Successfully uninstalled pbr-4.0.2\n",
      "  Found existing installation: mock 2.0.0\n",
      "    Uninstalling mock-2.0.0:\n",
      "      Successfully uninstalled mock-2.0.0\n",
      "  Found existing installation: PyYAML 3.12\n",
      "    Uninstalling PyYAML-3.12:\n",
      "      Successfully uninstalled PyYAML-3.12\n",
      "  Found existing installation: protobuf 3.5.2\n",
      "    Uninstalling protobuf-3.5.2:\n",
      "      Successfully uninstalled protobuf-3.5.2\n",
      "  Found existing installation: idna 2.6\n",
      "    Uninstalling idna-2.6:\n",
      "      Successfully uninstalled idna-2.6\n",
      "  Found existing installation: chardet 3.0.4\n",
      "    Uninstalling chardet-3.0.4:\n",
      "      Successfully uninstalled chardet-3.0.4\n",
      "  Found existing installation: urllib3 1.22\n",
      "    Uninstalling urllib3-1.22:\n",
      "      Successfully uninstalled urllib3-1.22\n",
      "  Found existing installation: certifi 2018.4.16\n",
      "    Uninstalling certifi-2018.4.16:\n",
      "      Successfully uninstalled certifi-2018.4.16\n",
      "  Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "  Found existing installation: google-apitools 0.5.10\n",
      "    Uninstalling google-apitools-0.5.10:\n",
      "      Successfully uninstalled google-apitools-0.5.10\n",
      "  Found existing installation: googleapis-common-protos 1.5.3\n",
      "    Uninstalling googleapis-common-protos-1.5.3:\n",
      "      Successfully uninstalled googleapis-common-protos-1.5.3\n",
      "  Found existing installation: cachetools 2.0.1\n",
      "    Uninstalling cachetools-2.0.1:\n",
      "      Successfully uninstalled cachetools-2.0.1\n",
      "  Found existing installation: google-auth 1.4.1\n",
      "    Uninstalling google-auth-1.4.1:\n",
      "      Successfully uninstalled google-auth-1.4.1\n",
      "  Found existing installation: google-auth-httplib2 0.0.3\n",
      "    Uninstalling google-auth-httplib2-0.0.3:\n",
      "      Successfully uninstalled google-auth-httplib2-0.0.3\n",
      "  Found existing installation: google-cloud-core 0.28.1\n",
      "    Uninstalling google-cloud-core-0.28.1:\n",
      "      Successfully uninstalled google-cloud-core-0.28.1\n",
      "  Found existing installation: google-cloud-bigquery 0.28.0\n",
      "    Uninstalling google-cloud-bigquery-0.28.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-0.28.0\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Found existing installation: ply 3.8\n",
      "    Uninstalling ply-3.8:\n",
      "      Successfully uninstalled ply-3.8\n",
      "  Found existing installation: google-gax 0.15.16\n",
      "    Uninstalling google-gax-0.15.16:\n",
      "      Successfully uninstalled google-gax-0.15.16\n",
      "  Found existing installation: grpc-google-iam-v1 0.11.4\n",
      "    Uninstalling grpc-google-iam-v1-0.11.4:\n",
      "      Successfully uninstalled grpc-google-iam-v1-0.11.4\n",
      "  Found existing installation: google-cloud-pubsub 0.30.1\n",
      "    Uninstalling google-cloud-pubsub-0.30.1:\n",
      "      Successfully uninstalled google-cloud-pubsub-0.30.1\n",
      "  Found existing installation: proto-google-cloud-datastore-v1 0.90.4\n",
      "    Uninstalling proto-google-cloud-datastore-v1-0.90.4:\n",
      "      Successfully uninstalled proto-google-cloud-datastore-v1-0.90.4\n",
      "  Found existing installation: googledatastore 7.0.1\n",
      "    Uninstalling googledatastore-7.0.1:\n",
      "      Successfully uninstalled googledatastore-7.0.1\n",
      "  Found existing installation: numpy 1.14.0\n",
      "    Uninstalling numpy-1.14.0:\n",
      "      Successfully uninstalled numpy-1.14.0\n",
      "Successfully installed apache-beam-2.4.0 avro-1.8.2 cachetools-2.1.0 certifi-2018.4.16 chardet-3.0.4 crcmod-1.7 dill-0.2.6 docopt-0.6.2 enum34-1.1.6 fasteners-0.14.1 funcsigs-1.0.2 future-0.16.0 futures-3.2.0 gapic-google-cloud-pubsub-v1-0.15.4 google-apitools-0.5.20 google-auth-1.5.0 google-auth-httplib2-0.0.3 google-cloud-bigquery-0.25.0 google-cloud-core-0.25.0 google-cloud-pubsub-0.26.0 google-gax-0.15.16 googleapis-common-protos-1.5.3 googledatastore-7.0.1 grpc-google-iam-v1-0.11.4 grpcio-1.12.1 hdfs-2.1.0 httplib2-0.9.2 idna-2.7 mock-2.0.0 monotonic-1.5 numpy-1.14.5 oauth2client-4.1.2 pbr-4.0.4 ply-3.8 proto-google-cloud-datastore-v1-0.90.4 proto-google-cloud-pubsub-v1-0.15.4 protobuf-3.5.2.post1 pyasn1-0.4.3 pyasn1-modules-0.2.1 pyvcf-0.6.8 pyyaml-3.12 requests-2.19.0 rsa-3.4.2 setuptools-39.2.0 six-1.11.0 tensorflow-transform-0.6.0 typing-3.6.4 urllib3-1.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "    DEPRECATION: Uninstalling a distutils installed project (crcmod) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n",
      "    DEPRECATION: Uninstalling a distutils installed project (pyyaml) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n",
      "    DEPRECATION: Uninstalling a distutils installed project (certifi) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n",
      "    DEPRECATION: Uninstalling a distutils installed project (future) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n",
      "You are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade --force tensorflow_transform==0.6.0 apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Restart the kernel</b> after you do a pip install (click on the <b>Reset</b> button in Datalab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-airflow==1.9.0\n",
      "apache-beam==2.4.0\n",
      "tensorflow==1.8.0\n",
      "tensorflow-transform==0.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip freeze | grep -e 'flow\\|beam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'eim-muse'\n",
    "PROJECT = 'eim-muse'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-13 17:12:53.337004. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Input source: BigQuery\n",
    "\n",
    "Get data from BigQuery but defer filtering etc. to Beam.\n",
    "Note that the dayofweek column is now strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "def create_query(phase, EVERY_N):\n",
    "  \"\"\"\n",
    "  phase: 1=train 2=valid\n",
    "  \"\"\"\n",
    "  base_query = \"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  `eim-muse.hallelujah_effect.full_hallelujah_trials_cleaned`\n",
    "  \"\"\"\n",
    "\n",
    "  if EVERY_N == None:\n",
    "    if phase < 2:\n",
    "      # Training\n",
    "      query = \"{0} WHERE MOD(FARM_FINGERPRINT(id), 10) < 7\".format(base_query)\n",
    "    else:\n",
    "      # Validation\n",
    "      query = \"{0} WHERE MOD(FARM_FINGERPRINT(id), 10) >= 8\".format(base_query)\n",
    "  else:\n",
    "      query = \"{0} WHERE MOD(FARM_FINGERPRINT(id), {1}) = {2}\".format(base_query, EVERY_N, phase)\n",
    "    \n",
    "  return query\n",
    "\n",
    "query = create_query(2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>most_engaged</th>\n",
       "      <th>most_enjoyed</th>\n",
       "      <th>artistic</th>\n",
       "      <th>fault</th>\n",
       "      <th>imagination</th>\n",
       "      <th>lazy</th>\n",
       "      <th>nervous</th>\n",
       "      <th>outgoing</th>\n",
       "      <th>reserved</th>\n",
       "      <th>stress</th>\n",
       "      <th>thorough</th>\n",
       "      <th>trusting</th>\n",
       "      <th>activity</th>\n",
       "      <th>engagement</th>\n",
       "      <th>familiarity</th>\n",
       "      <th>like_dislike</th>\n",
       "      <th>positivity</th>\n",
       "      <th>tension</th>\n",
       "      <th>terminal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.411765</td>\n",
       "      <td>2.647059</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>3.705882</td>\n",
       "      <td>3.470588</td>\n",
       "      <td>3.058824</td>\n",
       "      <td>3.529412</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>3.352941</td>\n",
       "      <td>3.294118</td>\n",
       "      <td>3.941176</td>\n",
       "      <td>3.235294</td>\n",
       "      <td>2.911765</td>\n",
       "      <td>3.441176</td>\n",
       "      <td>2.911765</td>\n",
       "      <td>3.235294</td>\n",
       "      <td>3.764706</td>\n",
       "      <td>2.735294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.760534</td>\n",
       "      <td>1.114741</td>\n",
       "      <td>1.114741</td>\n",
       "      <td>1.169464</td>\n",
       "      <td>1.053705</td>\n",
       "      <td>1.212678</td>\n",
       "      <td>1.124591</td>\n",
       "      <td>1.144038</td>\n",
       "      <td>0.717430</td>\n",
       "      <td>0.927520</td>\n",
       "      <td>0.931476</td>\n",
       "      <td>0.985184</td>\n",
       "      <td>0.658653</td>\n",
       "      <td>0.854891</td>\n",
       "      <td>1.564137</td>\n",
       "      <td>1.501336</td>\n",
       "      <td>1.504894</td>\n",
       "      <td>1.207522</td>\n",
       "      <td>1.074747</td>\n",
       "      <td>1.081772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             age  most_engaged  most_enjoyed   artistic      fault  \\\n",
       "count  34.000000     17.000000     17.000000  17.000000  17.000000   \n",
       "mean   23.411765      2.647059      2.352941   2.352941   2.882353   \n",
       "std    11.760534      1.114741      1.114741   1.169464   1.053705   \n",
       "min     5.000000      1.000000      1.000000   1.000000   1.000000   \n",
       "25%    17.250000      2.000000      1.000000   1.000000   2.000000   \n",
       "50%    21.000000      3.000000      2.000000   2.000000   3.000000   \n",
       "75%    31.750000      3.000000      3.000000   3.000000   4.000000   \n",
       "max    56.000000      4.000000      4.000000   4.000000   4.000000   \n",
       "\n",
       "       imagination       lazy    nervous   outgoing   reserved     stress  \\\n",
       "count    17.000000  17.000000  17.000000  17.000000  17.000000  17.000000   \n",
       "mean      3.705882   3.470588   3.058824   3.529412   2.882353   3.352941   \n",
       "std       1.212678   1.124591   1.144038   0.717430   0.927520   0.931476   \n",
       "min       1.000000   1.000000   1.000000   2.000000   2.000000   2.000000   \n",
       "25%       3.000000   3.000000   2.000000   3.000000   2.000000   3.000000   \n",
       "50%       4.000000   4.000000   3.000000   4.000000   3.000000   3.000000   \n",
       "75%       5.000000   4.000000   4.000000   4.000000   4.000000   4.000000   \n",
       "max       5.000000   5.000000   5.000000   4.000000   4.000000   5.000000   \n",
       "\n",
       "        thorough   trusting   activity  engagement  familiarity  like_dislike  \\\n",
       "count  17.000000  17.000000  34.000000   34.000000    34.000000     34.000000   \n",
       "mean    3.294118   3.941176   3.235294    2.911765     3.441176      2.911765   \n",
       "std     0.985184   0.658653   0.854891    1.564137     1.501336      1.504894   \n",
       "min     1.000000   2.000000   1.000000    1.000000     1.000000      1.000000   \n",
       "25%     3.000000   4.000000   3.000000    1.000000     2.250000      1.000000   \n",
       "50%     3.000000   4.000000   3.000000    3.000000     4.000000      3.000000   \n",
       "75%     4.000000   4.000000   4.000000    4.000000     5.000000      4.000000   \n",
       "max     5.000000   5.000000   4.000000    5.000000     5.000000      5.000000   \n",
       "\n",
       "       positivity    tension   terminal  \n",
       "count   34.000000  34.000000  34.000000  \n",
       "mean     3.235294   3.764706   2.735294  \n",
       "std      1.207522   1.074747   1.081772  \n",
       "min      1.000000   1.000000   1.000000  \n",
       "25%      2.250000   3.000000   2.000000  \n",
       "50%      3.500000   4.000000   3.000000  \n",
       "75%      4.000000   4.000000   4.000000  \n",
       "max      5.000000   5.000000   4.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = bq.Query(query).execute().result().to_dataframe()\n",
    "df_valid.head()\n",
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create ML dataset using tf.transform and Dataflow\n",
    "\n",
    "Let's use Cloud Dataflow to read in the BigQuery data and write it out as CSV files. Along the way, let's use tf.transform to do scaling and transforming. Using tf.transform allows us to save the metadata to ensure that the appropriate transformations get carried out during prediction as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%writefile requirements.txt\n",
    "tensorflow-transform==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job hallelujah-effect-features-180613-164159 ... hang on\n",
      "\n",
      "SELECT *\n",
      "FROM\n",
      "  `eim-muse.hallelujah_effect.full_hallelujah_trials_cleaned`\n",
      "   WHERE MOD(FARM_FINGERPRINT(id), 10) < 7\n",
      "{'tension': <tf.Tensor 'inputs/tension_copy:0' shape=(?,) dtype=float32>, 'like_dislike': <tf.Tensor 'inputs/like_dislike_copy:0' shape=(?,) dtype=float32>, 'language': <tf.Tensor 'inputs/language_copy:0' shape=(?,) dtype=string>, 'hallelujah_reaction': <tf.Tensor 'inputs/hallelujah_reaction_copy:0' shape=(?,) dtype=bool>, 'positivity': <tf.Tensor 'inputs/positivity_copy:0' shape=(?,) dtype=float32>, 'engagement': <tf.Tensor 'inputs/engagement_copy:0' shape=(?,) dtype=float32>, 'familiarity': <tf.Tensor 'inputs/familiarity_copy:0' shape=(?,) dtype=float32>, 'sex': <tf.Tensor 'inputs/sex_copy:0' shape=(?,) dtype=string>, 'location': <tf.Tensor 'inputs/location_copy:0' shape=(?,) dtype=string>, 'activity': <tf.Tensor 'inputs/activity_copy:0' shape=(?,) dtype=float32>, 'nationality': <tf.Tensor 'inputs/nationality_copy:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'inputs/age_copy:0' shape=(?,) dtype=float32>, 'hearing_impairments': <tf.Tensor 'inputs/hearing_impairments_copy:0' shape=(?,) dtype=bool>}\n",
      "{'tension': <tf.Tensor 'inputs/tension_copy:0' shape=(?,) dtype=float32>, 'like_dislike': <tf.Tensor 'inputs/like_dislike_copy:0' shape=(?,) dtype=float32>, 'language': <tf.Tensor 'inputs/language_copy:0' shape=(?,) dtype=string>, 'hallelujah_reaction': <tf.Tensor 'inputs/hallelujah_reaction_copy:0' shape=(?,) dtype=bool>, 'positivity': <tf.Tensor 'inputs/positivity_copy:0' shape=(?,) dtype=float32>, 'engagement': <tf.Tensor 'inputs/engagement_copy:0' shape=(?,) dtype=float32>, 'familiarity': <tf.Tensor 'inputs/familiarity_copy:0' shape=(?,) dtype=float32>, 'sex': <tf.Tensor 'inputs/sex_copy:0' shape=(?,) dtype=string>, 'location': <tf.Tensor 'inputs/location_copy:0' shape=(?,) dtype=string>, 'activity': <tf.Tensor 'inputs/activity_copy:0' shape=(?,) dtype=float32>, 'nationality': <tf.Tensor 'inputs/nationality_copy:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'inputs/age_copy:0' shape=(?,) dtype=float32>, 'hearing_impairments': <tf.Tensor 'inputs/hearing_impairments_copy:0' shape=(?,) dtype=bool>}\n",
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://eim-muse/analysis/hallelujah-effect/preproc_tft/tmp/tftransform_tmp/41021dc6b97b484488f762643b710cc2/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://eim-muse/analysis/hallelujah-effect/preproc_tft/tmp/tftransform_tmp/41021dc6b97b484488f762643b710cc2/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://eim-muse/analysis/hallelujah-effect/preproc_tft/tmp/tftransform_tmp/5a48534a791e42cf847114bbfb9652cb/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: gs://eim-muse/analysis/hallelujah-effect/preproc_tft/tmp/tftransform_tmp/5a48534a791e42cf847114bbfb9652cb/saved_model.pb\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-13 16:41:59.796012. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "\n",
    "def is_valid(inputs):\n",
    "#   try:\n",
    "#     pickup_longitude = inputs['pickuplon']\n",
    "#     dropoff_longitude = inputs['dropofflon']\n",
    "#     pickup_latitude = inputs['pickuplat']\n",
    "#     dropoff_latitude = inputs['dropofflat']\n",
    "#     hourofday = inputs['hourofday']\n",
    "#     dayofweek = inputs['dayofweek']\n",
    "#     passenger_count = inputs['passengers']\n",
    "#     fare_amount = inputs['fare_amount']\n",
    "#     return (fare_amount >= 2.5 and pickup_longitude > -78 and pickup_longitude < -70 \\\n",
    "#       and dropoff_longitude > -78 and dropoff_longitude < -70 and pickup_latitude > 37 \\\n",
    "#       and pickup_latitude < 45 and dropoff_latitude > 37 and dropoff_latitude < 45 \\\n",
    "#       and passenger_count > 0)\n",
    "#   except:\n",
    "#     return False\n",
    "  try:\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "  \n",
    "def preprocess_tft(inputs):\n",
    "      print(inputs)\n",
    "#       import datetime\n",
    "#       print inputs\n",
    "#       result = {}\n",
    "#       result['fare_amount'] = tf.identity(inputs['fare_amount'])     \n",
    "#       result['dayofweek'] = tft.string_to_int(inputs['dayofweek']) # builds a vocabulary\n",
    "#       result['hourofday'] = tf.identity(inputs['hourofday']) # pass through\n",
    "#       result['pickuplon'] = (tft.scale_to_0_1(inputs['pickuplon'])) # scaling numeric values\n",
    "#       result['pickuplat'] = (tft.scale_to_0_1(inputs['pickuplat']))\n",
    "#       result['dropofflon'] = (tft.scale_to_0_1(inputs['dropofflon']))\n",
    "#       result['dropofflat'] = (tft.scale_to_0_1(inputs['dropofflat']))\n",
    "#       result['passengers'] = tf.cast(inputs['passengers'], tf.float32) # a cast\n",
    "#       result['key'] = tf.as_string(tf.ones_like(inputs['passengers'])) # arbitrary TF func\n",
    "#       # engineered features\n",
    "#       latdiff = inputs['pickuplat'] - inputs['dropofflat']\n",
    "#       londiff = inputs['pickuplon'] - inputs['dropofflon']\n",
    "#       result['latdiff'] = tft.scale_to_0_1(latdiff)\n",
    "#       result['londiff'] = tft.scale_to_0_1(londiff)\n",
    "#       dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\n",
    "#       result['euclidean'] = dist\n",
    "#       return result\n",
    "      import datetime\n",
    "      print inputs\n",
    "      result = {}\n",
    "      result['age'] = tft.scale_to_0_1(inputs['age'])\n",
    "      result['activity'] = tft.scale_to_0_1(inputs['activity'])\n",
    "      result['hallelujah_reaction'] = tf.cast(inputs['hallelujah_reaction'], tf.int64)\n",
    "#       result['concentration'] = tft.scale_to_0_1(inputs['concentration'])\n",
    "      result['hearing_impairments'] = tf.cast(inputs['hearing_impairments'], tf.int64)\n",
    "      result['nationality'] = tf.identity(inputs['nationality'])\n",
    "      result['engagement'] = tft.scale_to_0_1(inputs['engagement'])\n",
    "      result['familiarity'] = tft.scale_to_0_1(inputs['familiarity'])\n",
    "      result['like_dislike'] = tft.scale_to_0_1(inputs['like_dislike'])\n",
    "      result['positivity'] = tft.scale_to_0_1(inputs['positivity'])\n",
    "      result['tension'] = tft.scale_to_0_1(inputs['tension'])\n",
    "      result['sex'] = tf.identity(inputs['sex'])\n",
    "      result['location'] = tf.identity(inputs['location'])\n",
    "      result['language'] = tf.identity(inputs['language'])\n",
    "#       result['dayofweek'] = tft.string_to_int(inputs['dayofweek']) # builds a vocabulary\n",
    "#       result['hourofday'] = tf.identity(inputs['hourofday']) # pass through\n",
    "#       result['pickuplon'] = (tft.scale_to_0_1(inputs['pickuplon'])) # scaling numeric values\n",
    "#       result['pickuplat'] = (tft.scale_to_0_1(inputs['pickuplat']))\n",
    "#       result['dropofflon'] = (tft.scale_to_0_1(inputs['dropofflon']))\n",
    "#       result['dropofflat'] = (tft.scale_to_0_1(inputs['dropofflat']))\n",
    "#       result['passengers'] = tf.cast(inputs['passengers'], tf.float32) # a cast\n",
    "#       result['key'] = tf.as_string(tf.ones_like(inputs['passengers'])) # arbitrary TF func\n",
    "#       # engineered features\n",
    "#       latdiff = inputs['pickuplat'] - inputs['dropofflat']\n",
    "#       londiff = inputs['pickuplon'] - inputs['dropofflon']\n",
    "#       result['latdiff'] = tft.scale_to_0_1(latdiff)\n",
    "#       result['londiff'] = tft.scale_to_0_1(londiff)\n",
    "#       dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\n",
    "#       result['euclidean'] = dist\n",
    "      return result\n",
    "\n",
    "def preprocess(in_test_mode):\n",
    "  import os\n",
    "  import os.path\n",
    "  import tempfile\n",
    "  from apache_beam.io import tfrecordio\n",
    "  from tensorflow_transform.coders import example_proto_coder\n",
    "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "  from tensorflow_transform.tf_metadata import dataset_schema\n",
    "  from tensorflow_transform.beam import tft_beam_io\n",
    "  from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "  job_name = 'hallelujah-effect-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')    \n",
    "  if in_test_mode:\n",
    "    import shutil\n",
    "    print 'Launching local job ... hang on'\n",
    "    OUTPUT_DIR = './preproc_tft'\n",
    "    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "    \n",
    "    # This could be an issue\n",
    "    EVERY_N = 5\n",
    "    \n",
    "  else:\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "    OUTPUT_DIR = 'gs://{0}/analysis/hallelujah-effect/preproc_tft/'.format(BUCKET)\n",
    "    import subprocess\n",
    "    subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
    "    \n",
    "    # This could be an issue\n",
    "    EVERY_N = 5\n",
    "    \n",
    "  # None of this EVERY_N business\n",
    "  EVERY_N = None\n",
    "    \n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT,\n",
    "    'max_num_workers': 24,\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True,\n",
    "    'requirements_file': 'requirements.txt'\n",
    "  }\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  if in_test_mode:\n",
    "    RUNNER = 'DirectRunner'\n",
    "  else:\n",
    "    RUNNER = 'DataflowRunner'\n",
    "\n",
    "  # set up metadata\n",
    "  raw_data_schema = {\n",
    "    colname : dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'nationality,sex,location,language'.split(',')\n",
    "  }\n",
    "  raw_data_schema.update({\n",
    "      colname : dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'age,activity,engagement,familiarity,like_dislike,positivity,tension'.split(',')\n",
    "    })\n",
    "#   raw_data_schema.update({\n",
    "#       colname : dataset_schema.ColumnSchema(tf.int64, [], dataset_schema.FixedColumnRepresentation())\n",
    "#                    for colname in 'activity,engagement,familiarity,like_dislike,positivity,tension'.split(',') # Excluding concentration for now\n",
    "#     })\n",
    "  raw_data_schema.update({\n",
    "      colname : dataset_schema.ColumnSchema(tf.bool, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'hallelujah_reaction,hearing_impairments'.split(',')\n",
    "    })\n",
    "  raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
    "  \n",
    "  # run Beam  \n",
    "  with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "    with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
    "      # save the raw data metadata\n",
    "      _ = (raw_data_metadata\n",
    "        | 'WriteInputMetadata' >> tft_beam_io.WriteMetadata(\n",
    "            os.path.join(OUTPUT_DIR, 'metadata/rawdata_metadata'),\n",
    "            pipeline=p))\n",
    "      \n",
    "      # analyze and transform training\n",
    "      this_query = create_query(1, EVERY_N)\n",
    "      print(this_query)\n",
    "      raw_data = (p \n",
    "        | 'train_read' >> beam.io.Read(beam.io.BigQuerySource(query=this_query, use_standard_sql=True))\n",
    "        | 'train_filter' >> beam.Filter(is_valid))\n",
    "\n",
    "      raw_dataset = (raw_data, raw_data_metadata)\n",
    "      transformed_dataset, transform_fn = (\n",
    "          raw_dataset | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))\n",
    "      transformed_data, transformed_metadata = transformed_dataset\n",
    "      _ = transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
    "          os.path.join(OUTPUT_DIR, 'train'),\n",
    "          file_name_suffix='.gz',\n",
    "          coder=example_proto_coder.ExampleProtoCoder(\n",
    "              transformed_metadata.schema))\n",
    "      \n",
    "      # transform eval data\n",
    "      raw_test_data = (p \n",
    "        | 'eval_read' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(2, EVERY_N), use_standard_sql=True))\n",
    "        | 'eval_filter' >> beam.Filter(is_valid))\n",
    "      \n",
    "      raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
    "      transformed_test_dataset = (\n",
    "          (raw_test_dataset, transform_fn) | beam_impl.TransformDataset())\n",
    "      transformed_test_data, _ = transformed_test_dataset\n",
    "      _ = transformed_test_data | 'WriteTestData' >> tfrecordio.WriteToTFRecord(\n",
    "          os.path.join(OUTPUT_DIR, 'eval'),\n",
    "          file_name_suffix='.gz',\n",
    "          coder=example_proto_coder.ExampleProtoCoder(\n",
    "              transformed_metadata.schema))\n",
    "      _ = (transform_fn\n",
    "           | 'WriteTransformFn' >>\n",
    "           transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'metadata')))\n",
    "\n",
    "preprocess(in_test_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2018-06-13T16:42:55Z  gs://eim-muse/analysis/hallelujah-effect/preproc_tft/\n",
      "      1139  2018-06-13T16:53:00Z  gs://eim-muse/analysis/hallelujah-effect/preproc_tft/eval-00000-of-00001.gz\n",
      "      1733  2018-06-13T16:50:56Z  gs://eim-muse/analysis/hallelujah-effect/preproc_tft/train-00000-of-00002.gz\n",
      "      4740  2018-06-13T16:50:56Z  gs://eim-muse/analysis/hallelujah-effect/preproc_tft/train-00001-of-00002.gz\n",
      "                                 gs://eim-muse/analysis/hallelujah-effect/preproc_tft/metadata/\n",
      "                                 gs://eim-muse/analysis/hallelujah-effect/preproc_tft/tmp/\n",
      "TOTAL: 4 objects, 7612 bytes (7.43 KiB)\n",
      "gs://eim-muse/analysis/hallelujah-effect/preproc_tft/metadata/\n",
      "gs://eim-muse/analysis/hallelujah-effect/preproc_tft/metadata/rawdata_metadata/\n",
      "gs://eim-muse/analysis/hallelujah-effect/preproc_tft/metadata/transform_fn/\n",
      "gs://eim-muse/analysis/hallelujah-effect/preproc_tft/metadata/transformed_metadata/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-13 17:13:10.653947. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# ls -l preproc_tft\n",
    "# ls preproc_tft/metadata\n",
    "gsutil ls -l gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/\n",
    "gsutil ls gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2> Train off preprocessed data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'layers_16_16_16'\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf ${PWD}/models/${MODEL_NAME}\n",
    "export PYTHONPATH=${PYTHONPATH}:$PWD/taxifare_tft\n",
    "python -m trainer.task \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\"  \\\n",
    "   --train_batch_size=128 \\\n",
    "   --output_dir=${PWD}/models/${MODEL_NAME} \\\n",
    "   --train_steps=50000 --eval_steps=1 --job-dir=/tmp \\\n",
    "   --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata\n",
    "   --hidden_units=\"16 16 16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 20767. Click <a href=\"/_proxy/55727/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20767"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-13 18:52:03.895964. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('./models/hallelujah-effect_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-13 17:21:02.649008. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "TensorBoard.stop(9225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://eim-muse/analysis/hallelujah-effect/models/hallelujah-effect_trained us-central1 hallelujah_effect180613_183612\n",
      "jobId: hallelujah_effect180613_183612\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://eim-muse/analysis/hallelujah-effect/models/hallelujah-effect_trained/events.out.tfevents.1528914662.cmle-training-master-9fca510eee-0-swmnt#1528914859376343...\n",
      "Removing gs://eim-muse/analysis/hallelujah-effect/models/hallelujah-effect_trained/graph.pbtxt#1528914858361092...\n",
      "Removing gs://eim-muse/analysis/hallelujah-effect/models/hallelujah-effect_trained/packages/047c61c78fdd06c345626ebb55193cc0c698f9112efc0a72f5584d715b2bec04/trainer-0.0.0.tar.gz#1528914650871896...\n",
      "/ [3/3 objects] 100% Done                                                       \n",
      "Operation completed over 3 objects.                                              \n",
      "Job [hallelujah_effect180613_183612] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe hallelujah_effect180613_183612\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs hallelujah_effect180613_183612\n",
      "bash: line 18: --train_batch_size=10: command not found\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-13 18:36:12.520514. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/analysis/hallelujah-effect/models/hallelujah-effect_trained\n",
    "JOBNAME=hallelujah_effect$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --package-path=${PWD}/taxifare/trainer \\\n",
    "   --module-name=trainer.task \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   --runtime-version=1.4 \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\" \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --train_steps=1000\n",
    "   --train_batch_size=10 \\\n",
    "   --eval_steps=100\n",
    "#    --config=hyperparam.yaml \\\n",
    "\n",
    "# --staging-bucket=gs://eim-muse-staging \\\n",
    "\n",
    "# export PYTHONPATH=${PYTHONPATH}:$PWD/taxifare_tft\n",
    "# python -m trainer.task \\\n",
    "#    --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "#    --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\"  \\\n",
    "#    --train_batch_size=10 \\\n",
    "#    --output_dir=\"gs://${BUCKET}/analysis/hallelujah-effect/models/hallelujah-effect_trained\" \\\n",
    "#    --train_steps=5000 --eval_steps=1 --job-dir=/tmp \\\n",
    "#    --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 11815. Click <a href=\"/_proxy/54141/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11815"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-13 17:50:19.229998. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "TensorBoard().start('gs://eim-muse/analysis/hallelujah-effect/models/hallelujah-effect_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-13 18:39:39.500024. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "TensorBoard().stop(11815)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2018-06-13T16:42:55Z  gs://eim-muse/analysis/hallelujah-effect/preproc_tft/\n",
      "      1139  2018-06-13T16:53:00Z  gs://eim-muse/analysis/hallelujah-effect/preproc_tft/eval-00000-of-00001.gz\n",
      "      1733  2018-06-13T16:50:56Z  gs://eim-muse/analysis/hallelujah-effect/preproc_tft/train-00000-of-00002.gz\n",
      "      4740  2018-06-13T16:50:56Z  gs://eim-muse/analysis/hallelujah-effect/preproc_tft/train-00001-of-00002.gz\n",
      "                                 gs://eim-muse/analysis/hallelujah-effect/preproc_tft/metadata/\n",
      "                                 gs://eim-muse/analysis/hallelujah-effect/preproc_tft/tmp/\n",
      "TOTAL: 4 objects, 7612 bytes (7.43 KiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-13 17:13:54.492991. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/test.json\n"
     ]
    }
   ],
   "source": [
    "%writefile /tmp/test.json\n",
    "{\"age\":\"29.0\",\"activity\":3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS_IDS  CLASSES  LOGISTIC               LOGITS                 PROBABILITIES\n",
      "[0]        [u'0']   [0.45960211753845215]  [-0.1619444638490677]  [0.5403978824615479, 0.45960214734077454]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: /usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2018-06-13 14:31:44.643762: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "model_dir=$(ls $PWD/hallelujah-effect_trained/export/exporter/)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=./hallelujah-effect_trained/export/exporter/${model_dir} \\\n",
    "    --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# To Do\n",
    "\n",
    "- LASSO to identify important features\n",
    "- Hyperparameter search\n",
    "- More plots and statistics from the dataset with which I'm working here\n",
    "- Bring in rows with missing values\n",
    "- Feature engineering (physiological signals, MIR, feature crosses, variable-width binning)\n",
    "- Include signals with good quality only in reaction range\n",
    "- Customize estimator to add additional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
