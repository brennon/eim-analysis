{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1> Exploring tf.transform </h1>\n",
    "\n",
    "While Pandas is fine for experimenting, for operationalization of your workflow, it is better to do preprocessing in Apache Beam. This will also help if you need to preprocess data in flight, since Apache Beam also allows for streaming.\n",
    "\n",
    "Only specific combinations of TensorFlow/Beam are supported by tf.transform. So make sure to get a combo that is.\n",
    "\n",
    "* TFT 0.6.0\n",
    "* TF 1.6 or higher\n",
    "* Apache Beam [GCP] 2.4.0 or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade --force tensorflow_transform==0.6.0 apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Restart the kernel</b> after you do a pip install (click on the <b>Reset</b> button in Datalab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-airflow==1.9.0\n",
      "apache-beam==2.4.0\n",
      "tensorflow==1.8.0\n",
      "tensorflow-transform==0.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip freeze | grep -e 'flow\\|beam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'eim-muse'\n",
    "PROJECT = 'eim-muse'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Input source: BigQuery\n",
    "\n",
    "Get data from BigQuery but defer filtering etc. to Beam.\n",
    "Note that the dayofweek column is now strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "def create_query(phase, EVERY_N):\n",
    "  \"\"\"\n",
    "  phase: 1=train 2=valid\n",
    "  \"\"\"\n",
    "  base_query = \"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  `eim-muse.hallelujah_effect.full_hallelujah_trials_cleaned`\n",
    "  \"\"\"\n",
    "\n",
    "  if EVERY_N == None:\n",
    "    if phase < 2:\n",
    "      # Training\n",
    "      query = \"{0} WHERE MOD(FARM_FINGERPRINT(id), 10) < 7\".format(base_query)\n",
    "    else:\n",
    "      # Validation\n",
    "      query = \"{0} WHERE MOD(FARM_FINGERPRINT(id), 10) >= 8\".format(base_query)\n",
    "  else:\n",
    "      query = \"{0} WHERE MOD(FARM_FINGERPRINT(id), {1}) = {2}\".format(base_query, EVERY_N, phase)\n",
    "    \n",
    "  return query\n",
    "\n",
    "query = create_query(2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>most_engaged</th>\n",
       "      <th>most_enjoyed</th>\n",
       "      <th>artistic</th>\n",
       "      <th>fault</th>\n",
       "      <th>imagination</th>\n",
       "      <th>lazy</th>\n",
       "      <th>nervous</th>\n",
       "      <th>outgoing</th>\n",
       "      <th>reserved</th>\n",
       "      <th>stress</th>\n",
       "      <th>thorough</th>\n",
       "      <th>trusting</th>\n",
       "      <th>activity</th>\n",
       "      <th>engagement</th>\n",
       "      <th>familiarity</th>\n",
       "      <th>like_dislike</th>\n",
       "      <th>positivity</th>\n",
       "      <th>tension</th>\n",
       "      <th>terminal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.411765</td>\n",
       "      <td>2.647059</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>3.705882</td>\n",
       "      <td>3.470588</td>\n",
       "      <td>3.058824</td>\n",
       "      <td>3.529412</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>3.352941</td>\n",
       "      <td>3.294118</td>\n",
       "      <td>3.941176</td>\n",
       "      <td>3.235294</td>\n",
       "      <td>2.911765</td>\n",
       "      <td>3.441176</td>\n",
       "      <td>2.911765</td>\n",
       "      <td>3.235294</td>\n",
       "      <td>3.764706</td>\n",
       "      <td>2.735294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.760534</td>\n",
       "      <td>1.114741</td>\n",
       "      <td>1.114741</td>\n",
       "      <td>1.169464</td>\n",
       "      <td>1.053705</td>\n",
       "      <td>1.212678</td>\n",
       "      <td>1.124591</td>\n",
       "      <td>1.144038</td>\n",
       "      <td>0.717430</td>\n",
       "      <td>0.927520</td>\n",
       "      <td>0.931476</td>\n",
       "      <td>0.985184</td>\n",
       "      <td>0.658653</td>\n",
       "      <td>0.854891</td>\n",
       "      <td>1.564137</td>\n",
       "      <td>1.501336</td>\n",
       "      <td>1.504894</td>\n",
       "      <td>1.207522</td>\n",
       "      <td>1.074747</td>\n",
       "      <td>1.081772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             age  most_engaged  most_enjoyed   artistic      fault  \\\n",
       "count  34.000000     17.000000     17.000000  17.000000  17.000000   \n",
       "mean   23.411765      2.647059      2.352941   2.352941   2.882353   \n",
       "std    11.760534      1.114741      1.114741   1.169464   1.053705   \n",
       "min     5.000000      1.000000      1.000000   1.000000   1.000000   \n",
       "25%    17.250000      2.000000      1.000000   1.000000   2.000000   \n",
       "50%    21.000000      3.000000      2.000000   2.000000   3.000000   \n",
       "75%    31.750000      3.000000      3.000000   3.000000   4.000000   \n",
       "max    56.000000      4.000000      4.000000   4.000000   4.000000   \n",
       "\n",
       "       imagination       lazy    nervous   outgoing   reserved     stress  \\\n",
       "count    17.000000  17.000000  17.000000  17.000000  17.000000  17.000000   \n",
       "mean      3.705882   3.470588   3.058824   3.529412   2.882353   3.352941   \n",
       "std       1.212678   1.124591   1.144038   0.717430   0.927520   0.931476   \n",
       "min       1.000000   1.000000   1.000000   2.000000   2.000000   2.000000   \n",
       "25%       3.000000   3.000000   2.000000   3.000000   2.000000   3.000000   \n",
       "50%       4.000000   4.000000   3.000000   4.000000   3.000000   3.000000   \n",
       "75%       5.000000   4.000000   4.000000   4.000000   4.000000   4.000000   \n",
       "max       5.000000   5.000000   5.000000   4.000000   4.000000   5.000000   \n",
       "\n",
       "        thorough   trusting   activity  engagement  familiarity  like_dislike  \\\n",
       "count  17.000000  17.000000  34.000000   34.000000    34.000000     34.000000   \n",
       "mean    3.294118   3.941176   3.235294    2.911765     3.441176      2.911765   \n",
       "std     0.985184   0.658653   0.854891    1.564137     1.501336      1.504894   \n",
       "min     1.000000   2.000000   1.000000    1.000000     1.000000      1.000000   \n",
       "25%     3.000000   4.000000   3.000000    1.000000     2.250000      1.000000   \n",
       "50%     3.000000   4.000000   3.000000    3.000000     4.000000      3.000000   \n",
       "75%     4.000000   4.000000   4.000000    4.000000     5.000000      4.000000   \n",
       "max     5.000000   5.000000   4.000000    5.000000     5.000000      5.000000   \n",
       "\n",
       "       positivity    tension   terminal  \n",
       "count   34.000000  34.000000  34.000000  \n",
       "mean     3.235294   3.764706   2.735294  \n",
       "std      1.207522   1.074747   1.081772  \n",
       "min      1.000000   1.000000   1.000000  \n",
       "25%      2.250000   3.000000   2.000000  \n",
       "50%      3.500000   4.000000   3.000000  \n",
       "75%      4.000000   4.000000   4.000000  \n",
       "max      5.000000   5.000000   4.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = bq.Query(query).execute().result().to_dataframe()\n",
    "df_valid.head()\n",
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create ML dataset using tf.transform and Dataflow\n",
    "\n",
    "Let's use Cloud Dataflow to read in the BigQuery data and write it out as CSV files. Along the way, let's use tf.transform to do scaling and transforming. Using tf.transform allows us to save the metadata to ensure that the appropriate transformations get carried out during prediction as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%writefile requirements.txt\n",
    "tensorflow-transform==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching local job ... hang on\n",
      "\n",
      "SELECT *\n",
      "FROM\n",
      "  `eim-muse.hallelujah_effect.full_hallelujah_trials_cleaned`\n",
      "   WHERE MOD(FARM_FINGERPRINT(id), 10) < 7\n",
      "{'tension': <tf.Tensor 'inputs/tension_copy:0' shape=(?,) dtype=float32>, 'like_dislike': <tf.Tensor 'inputs/like_dislike_copy:0' shape=(?,) dtype=float32>, 'language': <tf.Tensor 'inputs/language_copy:0' shape=(?,) dtype=string>, 'hallelujah_reaction': <tf.Tensor 'inputs/hallelujah_reaction_copy:0' shape=(?,) dtype=bool>, 'positivity': <tf.Tensor 'inputs/positivity_copy:0' shape=(?,) dtype=float32>, 'engagement': <tf.Tensor 'inputs/engagement_copy:0' shape=(?,) dtype=float32>, 'familiarity': <tf.Tensor 'inputs/familiarity_copy:0' shape=(?,) dtype=float32>, 'sex': <tf.Tensor 'inputs/sex_copy:0' shape=(?,) dtype=string>, 'location': <tf.Tensor 'inputs/location_copy:0' shape=(?,) dtype=string>, 'activity': <tf.Tensor 'inputs/activity_copy:0' shape=(?,) dtype=float32>, 'nationality': <tf.Tensor 'inputs/nationality_copy:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'inputs/age_copy:0' shape=(?,) dtype=float32>, 'hearing_impairments': <tf.Tensor 'inputs/hearing_impairments_copy:0' shape=(?,) dtype=bool>}\n",
      "{'tension': <tf.Tensor 'inputs/tension_copy:0' shape=(?,) dtype=float32>, 'like_dislike': <tf.Tensor 'inputs/like_dislike_copy:0' shape=(?,) dtype=float32>, 'language': <tf.Tensor 'inputs/language_copy:0' shape=(?,) dtype=string>, 'hallelujah_reaction': <tf.Tensor 'inputs/hallelujah_reaction_copy:0' shape=(?,) dtype=bool>, 'positivity': <tf.Tensor 'inputs/positivity_copy:0' shape=(?,) dtype=float32>, 'engagement': <tf.Tensor 'inputs/engagement_copy:0' shape=(?,) dtype=float32>, 'familiarity': <tf.Tensor 'inputs/familiarity_copy:0' shape=(?,) dtype=float32>, 'sex': <tf.Tensor 'inputs/sex_copy:0' shape=(?,) dtype=string>, 'location': <tf.Tensor 'inputs/location_copy:0' shape=(?,) dtype=string>, 'activity': <tf.Tensor 'inputs/activity_copy:0' shape=(?,) dtype=float32>, 'nationality': <tf.Tensor 'inputs/nationality_copy:0' shape=(?,) dtype=string>, 'age': <tf.Tensor 'inputs/age_copy:0' shape=(?,) dtype=float32>, 'hearing_impairments': <tf.Tensor 'inputs/hearing_impairments_copy:0' shape=(?,) dtype=bool>}\n",
      "tft.scale_to_0_1(inputs['age']):\n",
      "Tensor(\"scale_by_min_max_1/add:0\", shape=(?,), dtype=float32)\n",
      "tft.analyzers.mean(inputs['age']):\n",
      "Tensor(\"mean/truediv:0\", shape=(), dtype=float32)\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./preproc_tft-analyzer/tmp/tftransform_tmp/359a54e0a3f44eabaa7f9e39c842b62b/saved_model.pb\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./preproc_tft-analyzer/tmp/tftransform_tmp/cdc51641ca044258bfba2391a8520548/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:337: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported\n",
      "  pipeline.replace_all(_get_transform_overrides(pipeline.options))\n",
      "WARNING:root:Dataset eim-muse:temp_dataset_43e8238cf03e4653a7fa20cd1a71ba06 does not exist so we will create it as temporary with location=None\n",
      "WARNING:root:Dataset eim-muse:temp_dataset_fca4f0719c684929b4087040cf925f8d does not exist so we will create it as temporary with location=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./preproc_tft-analyzer/tmp/tftransform_tmp/c94814fb5b904f3199d75c47b229fee3/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./preproc_tft-analyzer/tmp/tftransform_tmp/c94814fb5b904f3199d75c47b229fee3/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/simplejson/encoder.py:286: DeprecationWarning: Interpreting naive datetime as local 2018-06-17 12:15:18.382956. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "\n",
    "def is_valid(inputs):\n",
    "#   try:\n",
    "#     pickup_longitude = inputs['pickuplon']\n",
    "#     dropoff_longitude = inputs['dropofflon']\n",
    "#     pickup_latitude = inputs['pickuplat']\n",
    "#     dropoff_latitude = inputs['dropofflat']\n",
    "#     hourofday = inputs['hourofday']\n",
    "#     dayofweek = inputs['dayofweek']\n",
    "#     passenger_count = inputs['passengers']\n",
    "#     fare_amount = inputs['fare_amount']\n",
    "#     return (fare_amount >= 2.5 and pickup_longitude > -78 and pickup_longitude < -70 \\\n",
    "#       and dropoff_longitude > -78 and dropoff_longitude < -70 and pickup_latitude > 37 \\\n",
    "#       and pickup_latitude < 45 and dropoff_latitude > 37 and dropoff_latitude < 45 \\\n",
    "#       and passenger_count > 0)\n",
    "#   except:\n",
    "#     return False\n",
    "  try:\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "  \n",
    "def preprocess_tft(inputs):\n",
    "      print(inputs)\n",
    "#       import datetime\n",
    "#       print inputs\n",
    "#       result = {}\n",
    "#       result['fare_amount'] = tf.identity(inputs['fare_amount'])     \n",
    "#       result['dayofweek'] = tft.string_to_int(inputs['dayofweek']) # builds a vocabulary\n",
    "#       result['hourofday'] = tf.identity(inputs['hourofday']) # pass through\n",
    "#       result['pickuplon'] = (tft.scale_to_0_1(inputs['pickuplon'])) # scaling numeric values\n",
    "#       result['pickuplat'] = (tft.scale_to_0_1(inputs['pickuplat']))\n",
    "#       result['dropofflon'] = (tft.scale_to_0_1(inputs['dropofflon']))\n",
    "#       result['dropofflat'] = (tft.scale_to_0_1(inputs['dropofflat']))\n",
    "#       result['passengers'] = tf.cast(inputs['passengers'], tf.float32) # a cast\n",
    "#       result['key'] = tf.as_string(tf.ones_like(inputs['passengers'])) # arbitrary TF func\n",
    "#       # engineered features\n",
    "#       latdiff = inputs['pickuplat'] - inputs['dropofflat']\n",
    "#       londiff = inputs['pickuplon'] - inputs['dropofflon']\n",
    "#       result['latdiff'] = tft.scale_to_0_1(latdiff)\n",
    "#       result['londiff'] = tft.scale_to_0_1(londiff)\n",
    "#       dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\n",
    "#       result['euclidean'] = dist\n",
    "#       return result\n",
    "      import datetime\n",
    "      print inputs\n",
    "      result = {}\n",
    "      result['age'] = tft.scale_to_0_1(inputs['age'])\n",
    "#       result['age_mean'] = tft.analyzers.mean(inputs['age'])\n",
    "      \n",
    "      print('tft.scale_to_0_1(inputs[\\'age\\']):')\n",
    "      print(tft.scale_to_0_1(inputs['age']))\n",
    "      print('tft.analyzers.mean(inputs[\\'age\\']):')\n",
    "      print(tft.analyzers.mean(inputs['age']))\n",
    "      \n",
    "      result['activity'] = tft.scale_to_0_1(inputs['activity'])\n",
    "      result['hallelujah_reaction'] = tf.cast(inputs['hallelujah_reaction'], tf.int64)\n",
    "#       result['concentration'] = tft.scale_to_0_1(inputs['concentration'])\n",
    "      result['hearing_impairments'] = tf.cast(inputs['hearing_impairments'], tf.int64)\n",
    "      result['nationality'] = tf.identity(inputs['nationality'])\n",
    "      result['engagement'] = tft.scale_to_0_1(inputs['engagement'])\n",
    "      result['familiarity'] = tft.scale_to_0_1(inputs['familiarity'])\n",
    "      result['like_dislike'] = tft.scale_to_0_1(inputs['like_dislike'])\n",
    "      result['positivity'] = tft.scale_to_0_1(inputs['positivity'])\n",
    "      result['tension'] = tft.scale_to_0_1(inputs['tension'])\n",
    "      result['sex'] = tf.identity(inputs['sex'])\n",
    "      result['location'] = tf.identity(inputs['location'])\n",
    "      result['language'] = tf.identity(inputs['language'])\n",
    "#       result['dayofweek'] = tft.string_to_int(inputs['dayofweek']) # builds a vocabulary\n",
    "#       result['hourofday'] = tf.identity(inputs['hourofday']) # pass through\n",
    "#       result['pickuplon'] = (tft.scale_to_0_1(inputs['pickuplon'])) # scaling numeric values\n",
    "#       result['pickuplat'] = (tft.scale_to_0_1(inputs['pickuplat']))\n",
    "#       result['dropofflon'] = (tft.scale_to_0_1(inputs['dropofflon']))\n",
    "#       result['dropofflat'] = (tft.scale_to_0_1(inputs['dropofflat']))\n",
    "#       result['passengers'] = tf.cast(inputs['passengers'], tf.float32) # a cast\n",
    "#       result['key'] = tf.as_string(tf.ones_like(inputs['passengers'])) # arbitrary TF func\n",
    "#       # engineered features\n",
    "#       latdiff = inputs['pickuplat'] - inputs['dropofflat']\n",
    "#       londiff = inputs['pickuplon'] - inputs['dropofflon']\n",
    "#       result['latdiff'] = tft.scale_to_0_1(latdiff)\n",
    "#       result['londiff'] = tft.scale_to_0_1(londiff)\n",
    "#       dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\n",
    "#       result['euclidean'] = dist\n",
    "      return result\n",
    "\n",
    "def preprocess(in_test_mode):\n",
    "  import os\n",
    "  import os.path\n",
    "  import tempfile\n",
    "  from apache_beam.io import tfrecordio\n",
    "  from tensorflow_transform.coders import example_proto_coder\n",
    "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "  from tensorflow_transform.tf_metadata import dataset_schema\n",
    "  from tensorflow_transform.beam import tft_beam_io\n",
    "  from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "  job_name = 'hallelujah-effect-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')    \n",
    "  if in_test_mode:\n",
    "    import shutil\n",
    "    print 'Launching local job ... hang on'\n",
    "    OUTPUT_DIR = './preproc_tft-analyzer'\n",
    "    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "    \n",
    "    # This could be an issue\n",
    "    EVERY_N = 5\n",
    "    \n",
    "  else:\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "    OUTPUT_DIR = 'gs://{0}/analysis/hallelujah-effect/preproc_tft/'.format(BUCKET)\n",
    "    import subprocess\n",
    "    subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
    "    \n",
    "    # This could be an issue\n",
    "    EVERY_N = 5\n",
    "    \n",
    "  # None of this EVERY_N business\n",
    "  EVERY_N = None\n",
    "    \n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT,\n",
    "    'max_num_workers': 24,\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True,\n",
    "    'requirements_file': 'requirements.txt'\n",
    "  }\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  if in_test_mode:\n",
    "    RUNNER = 'DirectRunner'\n",
    "  else:\n",
    "    RUNNER = 'DataflowRunner'\n",
    "\n",
    "  # set up metadata\n",
    "  raw_data_schema = {\n",
    "    colname : dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'nationality,sex,location,language'.split(',')\n",
    "  }\n",
    "  raw_data_schema.update({\n",
    "      colname : dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'age,activity,engagement,familiarity,like_dislike,positivity,tension'.split(',')\n",
    "    })\n",
    "#   raw_data_schema.update({\n",
    "#       colname : dataset_schema.ColumnSchema(tf.int64, [], dataset_schema.FixedColumnRepresentation())\n",
    "#                    for colname in 'activity,engagement,familiarity,like_dislike,positivity,tension'.split(',') # Excluding concentration for now\n",
    "#     })\n",
    "  raw_data_schema.update({\n",
    "      colname : dataset_schema.ColumnSchema(tf.bool, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'hallelujah_reaction,hearing_impairments'.split(',')\n",
    "    })\n",
    "  raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
    "  \n",
    "  # run Beam  \n",
    "  with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "    with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
    "      # save the raw data metadata\n",
    "      _ = (raw_data_metadata\n",
    "        | 'WriteInputMetadata' >> tft_beam_io.WriteMetadata(\n",
    "            os.path.join(OUTPUT_DIR, 'metadata/rawdata_metadata'),\n",
    "            pipeline=p))\n",
    "      \n",
    "      # analyze and transform training\n",
    "      this_query = create_query(1, EVERY_N)\n",
    "      print(this_query)\n",
    "      raw_data = (p \n",
    "        | 'train_read' >> beam.io.Read(beam.io.BigQuerySource(query=this_query, use_standard_sql=True))\n",
    "        | 'train_filter' >> beam.Filter(is_valid))\n",
    "\n",
    "      raw_dataset = (raw_data, raw_data_metadata)\n",
    "      transformed_dataset, transform_fn = (\n",
    "          raw_dataset | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))\n",
    "      transformed_data, transformed_metadata = transformed_dataset\n",
    "      _ = transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
    "          os.path.join(OUTPUT_DIR, 'train'),\n",
    "          file_name_suffix='.gz',\n",
    "          coder=example_proto_coder.ExampleProtoCoder(\n",
    "              transformed_metadata.schema))\n",
    "      \n",
    "      # transform eval data\n",
    "      raw_test_data = (p \n",
    "        | 'eval_read' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(2, EVERY_N), use_standard_sql=True))\n",
    "        | 'eval_filter' >> beam.Filter(is_valid))\n",
    "      \n",
    "      raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
    "      transformed_test_dataset = (\n",
    "          (raw_test_dataset, transform_fn) | beam_impl.TransformDataset())\n",
    "      transformed_test_data, _ = transformed_test_dataset\n",
    "      _ = transformed_test_data | 'WriteTestData' >> tfrecordio.WriteToTFRecord(\n",
    "          os.path.join(OUTPUT_DIR, 'eval'),\n",
    "          file_name_suffix='.gz',\n",
    "          coder=example_proto_coder.ExampleProtoCoder(\n",
    "              transformed_metadata.schema))\n",
    "      _ = (transform_fn\n",
    "           | 'WriteTransformFn' >>\n",
    "           transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'metadata')))\n",
    "\n",
    "preprocess(in_test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "# ls -l preproc_tft\n",
    "# ls preproc_tft/metadata\n",
    "gsutil ls -l gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/\n",
    "gsutil ls gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2> Train off preprocessed data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'layers_16_16_16'\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf ${PWD}/models/${MODEL_NAME}\n",
    "export PYTHONPATH=${PYTHONPATH}:$PWD/taxifare_tft\n",
    "python -m trainer.task \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\"  \\\n",
    "   --train_batch_size=128 \\\n",
    "   --output_dir=${PWD}/models/${MODEL_NAME} \\\n",
    "   --train_steps=50000 --eval_steps=1 --job-dir=/tmp \\\n",
    "   --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata\n",
    "   --hidden_units=\"16 16 16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://eim-muse/analysis/hallelujah-effect/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TensorBoard.stop(20767)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf ${PWD}/models/local-ml\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/taxifare_tft/trainer \\\n",
    "   --job-dir=${PWD}/models/local-ml \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\" \\\n",
    "   --train_steps=1000 \\\n",
    "   --train_batch_size=10 \\\n",
    "   --eval_steps=100 \\\n",
    "   --output_dir=${PWD}/models/local-ml \\\n",
    "   --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata/\n",
    "\n",
    "# %%bash\n",
    "# OUTDIR=gs://${BUCKET}/analysis/hallelujah-effect/models/hallelujah-effect_trained\n",
    "# JOBNAME=hallelujah_effect$(date -u +%y%m%d_%H%M%S)\n",
    "# echo $OUTDIR $REGION $JOBNAME\n",
    "# gsutil -m rm -rf $OUTDIR\n",
    "# gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "#    --region=$REGION \\\n",
    "#    --package-path=${PWD}/taxifare/trainer \\\n",
    "#    --module-name=trainer.task \\\n",
    "#    --job-dir=$OUTDIR \\\n",
    "#    --scale-tier=STANDARD_1 \\\n",
    "#    --runtime-version=1.4 \\\n",
    "#    -- \\\n",
    "#    --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "#    --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\" \\\n",
    "#    --output_dir=$OUTDIR \\\n",
    "#    --train_steps=1000\n",
    "#    --train_batch_size=10 \\\n",
    "#    --eval_steps=100\n",
    "#    --config=hyperparam.yaml \\\n",
    "\n",
    "# --staging-bucket=gs://eim-muse-staging \\\n",
    "\n",
    "# export PYTHONPATH=${PYTHONPATH}:$PWD/taxifare_tft\n",
    "# python -m trainer.task \\\n",
    "#    --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "#    --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\"  \\\n",
    "#    --train_batch_size=10 \\\n",
    "#    --output_dir=\"gs://${BUCKET}/analysis/hallelujah-effect/models/hallelujah-effect_trained\" \\\n",
    "#    --train_steps=5000 --eval_steps=1 --job-dir=/tmp \\\n",
    "#    --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://eim-muse/analysis/hallelujah-effect/models/hallelujah-effect_trained us-central1 hallelujah_effect180617_010329\n",
      "jobId: hallelujah_effect180617_010329\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://eim-muse/analysis/hallelujah-effect/models/hallelujah-effect_trained/packages/74e7ffc46410392056eed83f6f53c5f4e85bf7a7d304afd86525318557e46ecb/trainer-0.0.0.tar.gz#1529195303729699...\n",
      "/ [1/1 objects] 100% Done                                                       \r\n",
      "Operation completed over 1 objects.                                              \n",
      "Job [hallelujah_effect180617_010329] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe hallelujah_effect180617_010329\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs hallelujah_effect180617_010329\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "# rm -rf ${PWD}/models/local-ml\n",
    "# gcloud ml-engine local train \\\n",
    "#    --module-name=trainer.task \\\n",
    "#    --package-path=${PWD}/taxifare_tft/trainer \\\n",
    "#    --job-dir=${PWD}/models/local-ml \\\n",
    "#    -- \\\n",
    "#    --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "#    --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\" \\\n",
    "#    --train_steps=1000 \\\n",
    "#    --train_batch_size=10 \\\n",
    "#    --eval_steps=100 \\\n",
    "#    --output_dir=${PWD}/models/local-ml \\\n",
    "#    --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata/\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/analysis/hallelujah-effect/models/hallelujah-effect_trained\n",
    "JOBNAME=hallelujah_effect$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --package-path=${PWD}/taxifare_tft/trainer \\\n",
    "   --module-name=trainer.task \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   --runtime-version=1.4 \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\" \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --train_steps=1000 \\\n",
    "   --train_batch_size=10 \\\n",
    "   --eval_steps=100 \\\n",
    "   --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata/\n",
    "   \n",
    "# --config=hyperparam.yaml \\\n",
    "\n",
    "# --staging-bucket=gs://eim-muse-staging \\\n",
    "\n",
    "# export PYTHONPATH=${PYTHONPATH}:$PWD/taxifare_tft\n",
    "# python -m trainer.task \\\n",
    "#    --train_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/train*\" \\\n",
    "#    --eval_data_paths=\"gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/eval*\"  \\\n",
    "#    --train_batch_size=10 \\\n",
    "#    --output_dir=\"gs://${BUCKET}/analysis/hallelujah-effect/models/hallelujah-effect_trained\" \\\n",
    "#    --train_steps=5000 --eval_steps=1 --job-dir=/tmp \\\n",
    "#    --metadata_path=gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TensorBoard().start('./models/local-ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls gs://eim-muse/analysis/hallelujah-effect/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://${BUCKET}/analysis/hallelujah-effect/preproc_tft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%writefile /tmp/test.json\n",
    "{\"age\":\"29.0\",\"activity\":3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "model_dir=$(ls $PWD/hallelujah-effect_trained/export/exporter/)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=./hallelujah-effect_trained/export/exporter/${model_dir} \\\n",
    "    --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# To Do\n",
    "\n",
    "- LASSO to identify important features\n",
    "- Hyperparameter search\n",
    "- More plots and statistics from the dataset with which I'm working here\n",
    "- Bring in rows with missing values\n",
    "- Feature engineering (physiological signals, MIR, feature crosses, variable-width binning)\n",
    "- Include signals with good quality only in reaction range\n",
    "- Customize estimator to add additional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2016 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "from setuptools import find_packages\n",
      "from setuptools import setup\n",
      "\n",
      "REQUIRED_PACKAGES = [\n",
      "    'tensorflow_transform', 'hhasuwelafdmvsdf'\n",
      "]\n",
      "\n",
      "setup(\n",
      "    name='taxifare',\n",
      "    version='0.2',\n",
      "    author = 'Google',\n",
      "    author_email = 'training-feedback@cloud.google.com',\n",
      "    install_requires=REQUIRED_PACKAGES,\n",
      "    packages=find_packages(),\n",
      "    include_package_data=True,\n",
      "    description='CPB102 taxifare in Cloud ML',\n",
      "    requires=[]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "cat ${PWD}/taxifare_tft/trainer/setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
