{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/keremt/pytorch-entity-embeddings was super helpful with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skopt\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "from localhelpers import eim_train\n",
    "from localhelpers import eim_validate\n",
    "from localhelpers import emb_init\n",
    "from localhelpers import get_embs_dims\n",
    "from localhelpers import paired_ttest_5x2cv\n",
    "from localhelpers import preprocess\n",
    "from localhelpers import seed_everything\n",
    "from localhelpers import threshold_array\n",
    "\n",
    "# Classes\n",
    "from localhelpers import BaselineEstimator\n",
    "from localhelpers import CheckpointSaver\n",
    "from localhelpers import CustomEstimator\n",
    "from localhelpers import EimDataPreprocess\n",
    "from localhelpers import EimDataset\n",
    "from localhelpers import EimModel\n",
    "from localhelpers import EimModelData\n",
    "from localhelpers import ProgressCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate plots in notebook and suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed random number generators for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = int(time.time())\n",
    "print(f'Using random seed {random_seed}')\n",
    "seed_everything(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eim_csv = Path('..', '..', 'fastai', 'data', 'good_reaction_trials_categorified_q90_javier_all.csv')\n",
    "\n",
    "eim_df = pd.read_csv(eim_csv)\n",
    "train_df = eim_df[eim_df.song != 'hallelujah'].copy()\n",
    "valid_test_df = eim_df[eim_df.song == 'hallelujah'].copy()\n",
    "\n",
    "del eim_df\n",
    "\n",
    "# train_df = train_df.iloc[0:128]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['location', 'sex', 'musical_background', 'nationality', 'visual_impairments', 'hearing_impairments', \n",
    "        'musicpref_traditional_irish', 'musicpref_jazz', 'musicpref_folk', 'musicpref_hiphop', 'musicpref_classical', \n",
    "        'musicpref_none', 'musicpref_rock', 'musicpref_hip_hop', 'musicpref_world', 'musicpref_pop', 'musicpref_dance',\n",
    "        'language']\n",
    "\n",
    "conts = ['musical_expertise', 'age', 'control_activity', 'control_tension', 'control_engagement', \n",
    "         'control_positivity', 'control_like_dislike', 'song_chillsshiversthrills', 'song_activity', 'song_tension', \n",
    "         'song_familiarity', 'song_engagement', 'song_positivity', 'song_like_dislike', 'song_inspired', 'song_wonder', \n",
    "         'song_spirituality', 'song_thrills', 'song_chills', 'song_tenderness', 'song_nostalgia', 'song_goosebumps', \n",
    "         'song_overwhelmed', 'song_shivers', 'control_power', 'song_transcendence', 'song_sadness', 'song_joyfulactivation', \n",
    "         'song_peacefulness', 'song_power', 'personality_trusting', 'personality_artistic', 'personality_imagination', \n",
    "         'personality_reserved', 'personality_outgoing', 'personality_lazy', 'personality_stress', 'personality_nervous', \n",
    "         'personality_fault', 'personality_thorough', 'concentration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_test_df = pd.concat([train_df, valid_test_df],0)\n",
    "\n",
    "# Remove id, name, and item_description columns\n",
    "train_test_df = train_valid_test_df.drop(['_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_test_df.loc[train_valid_test_df.critical_reaction == False, 'critical_reaction'] = 0\n",
    "train_valid_test_df.loc[train_valid_test_df.critical_reaction == True, 'critical_reaction'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values and convert brand_name and category_name to categories\n",
    "train_valid_test_df = preprocess(train_valid_test_df, cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_test_df = train_valid_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values in categorical columns to 0, 1, 2, 3, ... from the top of the DataFrame down\n",
    "train_valid_test_df = EimDataPreprocess(train_valid_test_df, cats, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split combined DataFrame back out into train and test DataFrames\n",
    "train_df = train_valid_test_df.iloc[range(len(train_df))]\n",
    "valid_test_df = train_valid_test_df.iloc[range(len(train_df),len(train_valid_test_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pct, test_pct = .8, .2\n",
    "idxs = valid_test_df.index.values\n",
    "np.random.shuffle(idxs)\n",
    "split = int(np.floor(valid_pct * len(valid_test_df)))\n",
    "valid_idxs, test_idxs = idxs[:split], idxs[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_valid_df = train_df.copy()\n",
    "valid_df = valid_test_df.loc[valid_idxs, :]\n",
    "test_df = valid_test_df.loc[test_idxs, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment train and test DataFrames into input and output DataFrames\n",
    "train_input, train_y = train_df.drop('critical_reaction', 1), train_df.critical_reaction\n",
    "valid_input, valid_y = valid_df.drop('critical_reaction', 1), valid_df.critical_reaction\n",
    "test_input, test_y = test_df.drop('critical_reaction', 1), test_df.critical_reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the range of output values\n",
    "y_range = (train_y.min(), train_y.max())\n",
    "y_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of tuples of embedding dimensions\n",
    "# Tuples are (category_size, embedding_size), where embedding_size is the smaller of 50 and half the number of unique values in that category\n",
    "emb_szs = get_embs_dims(train_test_df, cats)\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary DataFrames to free up memory\n",
    "del train_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = EimModelData.from_data_frames('./tmp', train_input, valid_input, train_y, valid_y, \n",
    "                                           cats, conts, bs=len(train_input), test_df=test_input)\n",
    "emb_model = EimModel(emb_szs, len(conts), 0.04, 1, [1000, 500], 0.01, y_range=y_range, classify=True, use_bn=False)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "emb_model.to(device)\n",
    "emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_test = EimModelData.from_data_frames('./tmp', train_input, valid_input, train_y, valid_y, cats, conts, bs=len(train_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training dry-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(emb_model.parameters(), lr = 1e-2, weight_decay=1e-4)\n",
    "crit = torch.nn.functional.binary_cross_entropy\n",
    "\n",
    "epochs = 10\n",
    "losses = eim_train(emb_model, model_data, opt, crit, epochs, patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses['train'], label='Training Loss (Cross-Entropy)')\n",
    "plt.plot(losses['validation'], label='Validation Loss (Cross-Entropy)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "param_order = [\n",
    "    'learning_rate',\n",
    "    'weight_decay',\n",
    "    'layers',\n",
    "    'dropouts',\n",
    "    'embedding_dropout'\n",
    "]\n",
    "\n",
    "dimensions = [\n",
    "    Real(1e-8, 0.5, name='learning_rate'),\n",
    "    Real(1e-5, 1e-1, name='weight_decay'),\n",
    "    Categorical([(60, 60), (60, 30), (60, 30, 15), (30, 15)], name='layers'),\n",
    "    Real(0., 0.75, name='dropouts'),\n",
    "    Real(0., 0.75, name='embedding_dropout')\n",
    "]\n",
    "\n",
    "@skopt.utils.use_named_args(dimensions=dimensions)\n",
    "def objective(learning_rate=1e-4, weight_decay=0.04, epochs=10, layers=[10, 10], dropouts=[0.2, 0.2], \n",
    "              embedding_dropout=0.2, use_batch_norm=True):\n",
    "    \n",
    "    # Reset as much as possible\n",
    "    torch.cuda.empty_cache()\n",
    "    seed_everything(random_seed)\n",
    "\n",
    "    layers = list(layers)\n",
    "    \n",
    "    # Build model\n",
    "    emb_model = EimModel(emb_szs, len(conts), embedding_dropout, 1, layers, dropouts, y_range=y_range, \n",
    "                         classify=True, use_bn=use_batch_norm)\n",
    "    emb_model.to(device)\n",
    "    \n",
    "    opt = torch.optim.SGD(emb_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    crit = torch.nn.functional.binary_cross_entropy\n",
    "    losses = eim_train(emb_model, model_data, opt, crit, epochs, patience=25, print_output=False, save_best=False)\n",
    "    \n",
    "    return(min(losses['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "opt_checkpoint_path = Path('.', 'opt_checkpoint.pkl')\n",
    "\n",
    "resume_optimization = False\n",
    "previous_x0, previous_y0 = None, None\n",
    "n_calls, n_called = 100, 0\n",
    "\n",
    "if resume_optimization:\n",
    "    from skopt import load\n",
    "    \n",
    "    if opt_checkpoint_path.exists:\n",
    "        previous_res = load(opt_checkpoint_path)\n",
    "        previous_x0 = previous_res.x_iters\n",
    "        previous_y0 = previous_res.func_vals\n",
    "        n_called = len(previous_x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_calls - n_called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = CheckpointSaver(opt_checkpoint_path)\n",
    "progress_callback = ProgressCallback(n_calls - n_called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results = skopt.gp_minimize(objective,\n",
    "                                dimensions,\n",
    "                                n_calls=n_calls - n_called,\n",
    "                                random_state=random_seed,\n",
    "                                x0=previous_x0,\n",
    "                                y0=previous_y0,\n",
    "                                callback=[progress_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save hyperparameter optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import dump\n",
    "dump(opt_results, opt_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import load\n",
    "opt_results = load(opt_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best loss: {}'.format(opt_results.fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters:')\n",
    "opt_params = dict(zip(param_order, opt_results.x))\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with these parameters until we start to diverge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(random_seed)\n",
    "epochs = 5000\n",
    "emb_model = EimModel(emb_szs, len(conts), opt_params['embedding_dropout'], 1, list(opt_params['layers']), \n",
    "                     opt_params['dropouts'], y_range=y_range, classify=True, \n",
    "                     use_bn=True)\n",
    "emb_model.to(device)\n",
    "opt = torch.optim.SGD(emb_model.parameters(), lr=opt_params['learning_rate'], weight_decay=opt_params['weight_decay'])\n",
    "losses = eim_train(emb_model, model_data, opt, crit, epochs, save_best=True, print_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses['train'], label='Training Loss (Cross-Entropy)')\n",
    "plt.plot(losses['validation'], label='Validation Loss (Cross-Entropy)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = EimModel(emb_szs, len(conts), opt_params['embedding_dropout'], 1, list(opt_params['layers']), \n",
    "                     opt_params['dropouts'], y_range=y_range, classify=True, \n",
    "                     use_bn=True)\n",
    "emb_model.to(device)\n",
    "emb_model.load_state_dict(torch.load(Path('.', 'best_model.pkl')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate validation set against model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(model_data.val_dl))\n",
    "\n",
    "emb_model.eval()\n",
    "\n",
    "# get inputs\n",
    "x_cats, x_conts, y = data\n",
    "\n",
    "# wrap with variable\n",
    "x_cats = torch.LongTensor(x_cats).to(device)\n",
    "x_conts = torch.FloatTensor(x_conts).to(device)\n",
    "y = torch.FloatTensor(y).to(device)\n",
    "x_cats.requires_grad = False\n",
    "x_conts.requires_grad = False\n",
    "y.requires_grad = False\n",
    "\n",
    "val_outputs = emb_model(x_cats, x_conts).cpu().detach().numpy()\n",
    "val_y = y.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot validation AUC-PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "average_precision = average_precision_score(val_y, val_outputs)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(val_y, val_outputs)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine threshold for optimum F-beta / Calculate F-beta score for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_array(threshold, array):\n",
    "    array = array.copy()\n",
    "    array[array >= threshold] = 1.\n",
    "    array[array < threshold] = 0.\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fbeta = 0.0\n",
    "best_fbeta_thresh = 0.0\n",
    "for thresh in thresholds:\n",
    "    val_outputs_thresh = threshold_array(thresh, val_outputs)\n",
    "    fb = fbeta_score(val_y, val_outputs_thresh, 0.5, average='weighted')\n",
    "    if fb >= best_fbeta:\n",
    "        best_fbeta = fb\n",
    "        best_fbeta_thresh = thresh\n",
    "\n",
    "print('Best F_0.5 {} (at threshold {})'.format(best_fbeta, best_fbeta_thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate test set against model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = None\n",
    "for data in iter(model_data.test_dl):\n",
    "\n",
    "    emb_model.eval()\n",
    "\n",
    "    # get inputs\n",
    "    x_cats, x_conts, y = data\n",
    "\n",
    "    # wrap with variable\n",
    "    x_cats = torch.LongTensor(x_cats).to(device)\n",
    "    x_conts = torch.FloatTensor(x_conts).to(device)\n",
    "    x_cats.requires_grad = False\n",
    "    x_conts.requires_grad = False\n",
    "\n",
    "    outputs = emb_model(x_cats, x_conts).cpu().detach().numpy()\n",
    "    if test_outputs is not None:\n",
    "        test_outputs = np.concatenate((test_outputs, outputs))\n",
    "    else:\n",
    "        test_outputs = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot test AUC-PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(test_y, test_outputs)\n",
    "\n",
    "# print('Average precision-recall score: {0:0.2f}'.format(\n",
    "#       average_precision))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(test_y, test_outputs)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine threshold for optimum F-beta / Calculate F-beta score for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fbeta = 0.0\n",
    "best_fbeta_thresh = 0.0\n",
    "for thresh in thresholds:\n",
    "    test_outputs_thresh = threshold_array(thresh, test_outputs)\n",
    "    fb = fbeta_score(test_y, test_outputs_thresh, 0.5, average='weighted')\n",
    "    if fb >= best_fbeta:\n",
    "        best_fbeta = fb\n",
    "        best_fbeta_thresh = thresh\n",
    "\n",
    "print('Best F_0.5 {} (at threshold {})'.format(best_fbeta, best_fbeta_thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect CV results from multiple runs against validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results against baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_estimator = CustomEstimator(cats, conts, emb_szs, opt_params['embedding_dropout'], \n",
    "                                   opt_params['layers'], opt_params['dropouts'], y_range, \n",
    "                                   True, opt_params['learning_rate'], opt_params['weight_decay'], \n",
    "                                   epochs=100, optimization_n=100, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_estimator = BaselineEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = paired_ttest_5x2cv(custom_estimator, baseline_estimator, train_input, \n",
    "                            pd.concat([valid_input, test_input], ignore_index=True), \n",
    "                            train_y, pd.concat([valid_y, test_y], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-tailed t-test p-value\n",
    "cv_res['pvalue'] / 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-tailed t-test confirms significant difference between average $F_{0.5}$ scores of these two classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
